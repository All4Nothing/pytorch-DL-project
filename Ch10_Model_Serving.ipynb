{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNHLHpaPa5+BK8+uoVHWiLj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/All4Nothing/pytorch-DL-project/blob/main/Ch10_Model_Serving.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Serving"
      ],
      "metadata": {
        "id": "t7FBREFziJdQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Train & Save"
      ],
      "metadata": {
        "id": "LwmXTxRNiNYZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "G0Q5a2vriESg"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class ConvNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ConvNet, self).__init__()\n",
        "        self.cn1 = nn.Conv2d(1, 16, 3, 1)\n",
        "        self.cn2 = nn.Conv2d(16, 32, 3, 1)\n",
        "        self.dp1 = nn.Dropout2d(0.10)\n",
        "        self.dp2 = nn.Dropout2d(0.25)\n",
        "        self.fc1 = nn.Linear(4608, 64) # 4608 is basically 12 X 12 X 32\n",
        "        self.fc2 = nn.Linear(64, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.cn1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.cn2(x)\n",
        "        x = F.relu(x)\n",
        "        x = F.max_pool2d(x, 2)\n",
        "        x = self.dp1(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.fc1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.dp2(x)\n",
        "        x = self.fc2(x)\n",
        "        op = F.log_softmax(x, dim=1)\n",
        "        return op"
      ],
      "metadata": {
        "id": "U9Rg92wfiSbg"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, device, train_dataloader, optim, epoch):\n",
        "    model.train()\n",
        "    for b_i, (X, y) in enumerate(train_dataloader):\n",
        "        X, y = X.to(device), y.to(device)\n",
        "        optim.zero_grad()\n",
        "        pred_prob = model(X)\n",
        "        loss = F.nll_loss(pred_prob, y) # nll is the negative likelihood loss\n",
        "        loss.backward()\n",
        "        optim.step()\n",
        "        if b_i % 10 == 0:\n",
        "            print('epoch: {} [{}/{} ({:.0f}%)]\\t training loss: {:.6f}'.format(\n",
        "                epoch, b_i * len(X), len(train_dataloader.dataset),\n",
        "                100. * b_i / len(train_dataloader), loss.item()))"
      ],
      "metadata": {
        "id": "tHOsBQTfiTdu"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test(model, device, test_dataloader):\n",
        "    model.eval()\n",
        "    loss = 0\n",
        "    success = 0\n",
        "    with torch.no_grad():\n",
        "        for X, y in test_dataloader:\n",
        "            X, y = X.to(device), y.to(device)\n",
        "            pred_prob = model(X)\n",
        "            loss += F.nll_loss(pred_prob, y, reduction='sum').item()  # loss summed across the batch\n",
        "            pred = pred_prob.argmax(dim=1, keepdim=True)  # us argmax to get the most likely prediction\n",
        "            success += pred.eq(y.view_as(pred)).sum().item()\n",
        "\n",
        "    loss /= len(test_dataloader.dataset)\n",
        "\n",
        "    print('\\nTest dataset: Overall Loss: {:.4f}, Overall Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "        loss, success, len(test_dataloader.dataset),\n",
        "        100. * success / len(test_dataloader.dataset)))"
      ],
      "metadata": {
        "id": "9RQWSqo5iUhg"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The mean and standard deviation values are calculated as the mean of all pixel values of all images in the training dataset\n",
        "train_dataloader = torch.utils.data.DataLoader(\n",
        "    datasets.MNIST('../data', train=True, download=True,\n",
        "                   transform=transforms.Compose([\n",
        "                       transforms.ToTensor(),\n",
        "                       transforms.Normalize((0.1302,), (0.3069,))])), # train_X.mean()/256. and train_X.std()/256.\n",
        "    batch_size=32, shuffle=True)\n",
        "\n",
        "test_dataloader = torch.utils.data.DataLoader(\n",
        "    datasets.MNIST('../data', train=False,\n",
        "                   transform=transforms.Compose([\n",
        "                       transforms.ToTensor(),\n",
        "                       transforms.Normalize((0.1302,), (0.3069,))\n",
        "                   ])),\n",
        "    batch_size=500, shuffle=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gFsujjkhiVpX",
        "outputId": "c6dd901a-99c4-42d5-e157-768265d058b8"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ../data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9912422/9912422 [00:00<00:00, 164184211.91it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ../data/MNIST/raw/train-images-idx3-ubyte.gz to ../data/MNIST/raw\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ../data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 28881/28881 [00:00<00:00, 121866895.20it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ../data/MNIST/raw/train-labels-idx1-ubyte.gz to ../data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ../data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1648877/1648877 [00:00<00:00, 52913027.22it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ../data/MNIST/raw/t10k-images-idx3-ubyte.gz to ../data/MNIST/raw\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ../data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4542/4542 [00:00<00:00, 20684613.21it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ../data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ../data/MNIST/raw\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(0)\n",
        "device = torch.device(\"cpu\")\n",
        "\n",
        "model = ConvNet()\n",
        "optimizer = optim.Adadelta(model.parameters(), lr=0.5)"
      ],
      "metadata": {
        "id": "Px6DTArdiW1W"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(1, 3):\n",
        "    train(model, device, train_dataloader, optimizer, epoch)\n",
        "    test(model, device, test_dataloader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bAh1nRfIiYF2",
        "outputId": "40adf1ae-f22c-4e75-f0ae-ab415b0d6be4"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py:1345: UserWarning: dropout2d: Received a 2-D input to dropout2d, which is deprecated and will result in an error in a future release. To retain the behavior and silence this warning, please use dropout instead. Note that dropout2d exists to provide channel-wise dropout on inputs with 2 spatial dimensions, a channel dimension, and an optional batch dimension (i.e. 3D or 4D inputs).\n",
            "  warnings.warn(warn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 1 [0/60000 (0%)]\t training loss: 2.310609\n",
            "epoch: 1 [320/60000 (1%)]\t training loss: 1.924132\n",
            "epoch: 1 [640/60000 (1%)]\t training loss: 1.313337\n",
            "epoch: 1 [960/60000 (2%)]\t training loss: 0.796470\n",
            "epoch: 1 [1280/60000 (2%)]\t training loss: 0.819801\n",
            "epoch: 1 [1600/60000 (3%)]\t training loss: 0.678430\n",
            "epoch: 1 [1920/60000 (3%)]\t training loss: 0.477187\n",
            "epoch: 1 [2240/60000 (4%)]\t training loss: 0.529062\n",
            "epoch: 1 [2560/60000 (4%)]\t training loss: 0.468848\n",
            "epoch: 1 [2880/60000 (5%)]\t training loss: 0.242855\n",
            "epoch: 1 [3200/60000 (5%)]\t training loss: 0.520190\n",
            "epoch: 1 [3520/60000 (6%)]\t training loss: 0.266329\n",
            "epoch: 1 [3840/60000 (6%)]\t training loss: 0.473294\n",
            "epoch: 1 [4160/60000 (7%)]\t training loss: 0.422828\n",
            "epoch: 1 [4480/60000 (7%)]\t training loss: 0.322728\n",
            "epoch: 1 [4800/60000 (8%)]\t training loss: 0.498349\n",
            "epoch: 1 [5120/60000 (9%)]\t training loss: 0.149704\n",
            "epoch: 1 [5440/60000 (9%)]\t training loss: 0.369297\n",
            "epoch: 1 [5760/60000 (10%)]\t training loss: 0.085426\n",
            "epoch: 1 [6080/60000 (10%)]\t training loss: 0.175981\n",
            "epoch: 1 [6400/60000 (11%)]\t training loss: 0.273135\n",
            "epoch: 1 [6720/60000 (11%)]\t training loss: 0.081465\n",
            "epoch: 1 [7040/60000 (12%)]\t training loss: 0.386300\n",
            "epoch: 1 [7360/60000 (12%)]\t training loss: 0.040537\n",
            "epoch: 1 [7680/60000 (13%)]\t training loss: 0.383194\n",
            "epoch: 1 [8000/60000 (13%)]\t training loss: 0.221456\n",
            "epoch: 1 [8320/60000 (14%)]\t training loss: 0.343185\n",
            "epoch: 1 [8640/60000 (14%)]\t training loss: 0.211232\n",
            "epoch: 1 [8960/60000 (15%)]\t training loss: 0.057373\n",
            "epoch: 1 [9280/60000 (15%)]\t training loss: 0.335909\n",
            "epoch: 1 [9600/60000 (16%)]\t training loss: 0.213373\n",
            "epoch: 1 [9920/60000 (17%)]\t training loss: 0.214295\n",
            "epoch: 1 [10240/60000 (17%)]\t training loss: 0.209615\n",
            "epoch: 1 [10560/60000 (18%)]\t training loss: 0.407195\n",
            "epoch: 1 [10880/60000 (18%)]\t training loss: 0.088149\n",
            "epoch: 1 [11200/60000 (19%)]\t training loss: 0.050920\n",
            "epoch: 1 [11520/60000 (19%)]\t training loss: 0.248843\n",
            "epoch: 1 [11840/60000 (20%)]\t training loss: 0.056553\n",
            "epoch: 1 [12160/60000 (20%)]\t training loss: 0.412060\n",
            "epoch: 1 [12480/60000 (21%)]\t training loss: 0.035822\n",
            "epoch: 1 [12800/60000 (21%)]\t training loss: 0.082240\n",
            "epoch: 1 [13120/60000 (22%)]\t training loss: 0.106317\n",
            "epoch: 1 [13440/60000 (22%)]\t training loss: 0.171955\n",
            "epoch: 1 [13760/60000 (23%)]\t training loss: 0.074246\n",
            "epoch: 1 [14080/60000 (23%)]\t training loss: 0.054975\n",
            "epoch: 1 [14400/60000 (24%)]\t training loss: 0.089665\n",
            "epoch: 1 [14720/60000 (25%)]\t training loss: 0.171017\n",
            "epoch: 1 [15040/60000 (25%)]\t training loss: 0.372591\n",
            "epoch: 1 [15360/60000 (26%)]\t training loss: 0.073247\n",
            "epoch: 1 [15680/60000 (26%)]\t training loss: 0.100345\n",
            "epoch: 1 [16000/60000 (27%)]\t training loss: 0.088487\n",
            "epoch: 1 [16320/60000 (27%)]\t training loss: 0.149706\n",
            "epoch: 1 [16640/60000 (28%)]\t training loss: 0.032775\n",
            "epoch: 1 [16960/60000 (28%)]\t training loss: 0.330667\n",
            "epoch: 1 [17280/60000 (29%)]\t training loss: 0.045406\n",
            "epoch: 1 [17600/60000 (29%)]\t training loss: 0.027192\n",
            "epoch: 1 [17920/60000 (30%)]\t training loss: 0.045530\n",
            "epoch: 1 [18240/60000 (30%)]\t training loss: 0.029942\n",
            "epoch: 1 [18560/60000 (31%)]\t training loss: 0.050827\n",
            "epoch: 1 [18880/60000 (31%)]\t training loss: 0.228748\n",
            "epoch: 1 [19200/60000 (32%)]\t training loss: 0.107526\n",
            "epoch: 1 [19520/60000 (33%)]\t training loss: 0.057280\n",
            "epoch: 1 [19840/60000 (33%)]\t training loss: 0.077947\n",
            "epoch: 1 [20160/60000 (34%)]\t training loss: 0.112361\n",
            "epoch: 1 [20480/60000 (34%)]\t training loss: 0.072645\n",
            "epoch: 1 [20800/60000 (35%)]\t training loss: 0.202532\n",
            "epoch: 1 [21120/60000 (35%)]\t training loss: 0.330348\n",
            "epoch: 1 [21440/60000 (36%)]\t training loss: 0.251984\n",
            "epoch: 1 [21760/60000 (36%)]\t training loss: 0.095232\n",
            "epoch: 1 [22080/60000 (37%)]\t training loss: 0.101939\n",
            "epoch: 1 [22400/60000 (37%)]\t training loss: 0.141548\n",
            "epoch: 1 [22720/60000 (38%)]\t training loss: 0.273435\n",
            "epoch: 1 [23040/60000 (38%)]\t training loss: 0.080547\n",
            "epoch: 1 [23360/60000 (39%)]\t training loss: 0.137531\n",
            "epoch: 1 [23680/60000 (39%)]\t training loss: 0.263773\n",
            "epoch: 1 [24000/60000 (40%)]\t training loss: 0.176301\n",
            "epoch: 1 [24320/60000 (41%)]\t training loss: 0.064812\n",
            "epoch: 1 [24640/60000 (41%)]\t training loss: 0.137117\n",
            "epoch: 1 [24960/60000 (42%)]\t training loss: 0.029184\n",
            "epoch: 1 [25280/60000 (42%)]\t training loss: 0.018340\n",
            "epoch: 1 [25600/60000 (43%)]\t training loss: 0.606099\n",
            "epoch: 1 [25920/60000 (43%)]\t training loss: 0.522304\n",
            "epoch: 1 [26240/60000 (44%)]\t training loss: 0.114520\n",
            "epoch: 1 [26560/60000 (44%)]\t training loss: 0.042568\n",
            "epoch: 1 [26880/60000 (45%)]\t training loss: 0.178105\n",
            "epoch: 1 [27200/60000 (45%)]\t training loss: 0.016068\n",
            "epoch: 1 [27520/60000 (46%)]\t training loss: 0.037695\n",
            "epoch: 1 [27840/60000 (46%)]\t training loss: 0.143481\n",
            "epoch: 1 [28160/60000 (47%)]\t training loss: 0.036192\n",
            "epoch: 1 [28480/60000 (47%)]\t training loss: 0.325821\n",
            "epoch: 1 [28800/60000 (48%)]\t training loss: 0.115090\n",
            "epoch: 1 [29120/60000 (49%)]\t training loss: 0.209149\n",
            "epoch: 1 [29440/60000 (49%)]\t training loss: 0.011254\n",
            "epoch: 1 [29760/60000 (50%)]\t training loss: 0.047660\n",
            "epoch: 1 [30080/60000 (50%)]\t training loss: 0.552331\n",
            "epoch: 1 [30400/60000 (51%)]\t training loss: 0.130882\n",
            "epoch: 1 [30720/60000 (51%)]\t training loss: 0.190394\n",
            "epoch: 1 [31040/60000 (52%)]\t training loss: 0.091782\n",
            "epoch: 1 [31360/60000 (52%)]\t training loss: 0.070341\n",
            "epoch: 1 [31680/60000 (53%)]\t training loss: 0.020346\n",
            "epoch: 1 [32000/60000 (53%)]\t training loss: 0.010616\n",
            "epoch: 1 [32320/60000 (54%)]\t training loss: 0.125683\n",
            "epoch: 1 [32640/60000 (54%)]\t training loss: 0.103808\n",
            "epoch: 1 [32960/60000 (55%)]\t training loss: 0.073545\n",
            "epoch: 1 [33280/60000 (55%)]\t training loss: 0.237079\n",
            "epoch: 1 [33600/60000 (56%)]\t training loss: 0.218271\n",
            "epoch: 1 [33920/60000 (57%)]\t training loss: 0.018967\n",
            "epoch: 1 [34240/60000 (57%)]\t training loss: 0.027074\n",
            "epoch: 1 [34560/60000 (58%)]\t training loss: 0.002536\n",
            "epoch: 1 [34880/60000 (58%)]\t training loss: 0.147322\n",
            "epoch: 1 [35200/60000 (59%)]\t training loss: 0.234867\n",
            "epoch: 1 [35520/60000 (59%)]\t training loss: 0.130815\n",
            "epoch: 1 [35840/60000 (60%)]\t training loss: 0.031518\n",
            "epoch: 1 [36160/60000 (60%)]\t training loss: 0.167266\n",
            "epoch: 1 [36480/60000 (61%)]\t training loss: 0.110819\n",
            "epoch: 1 [36800/60000 (61%)]\t training loss: 0.146991\n",
            "epoch: 1 [37120/60000 (62%)]\t training loss: 0.047040\n",
            "epoch: 1 [37440/60000 (62%)]\t training loss: 0.202389\n",
            "epoch: 1 [37760/60000 (63%)]\t training loss: 0.043542\n",
            "epoch: 1 [38080/60000 (63%)]\t training loss: 0.110692\n",
            "epoch: 1 [38400/60000 (64%)]\t training loss: 0.041221\n",
            "epoch: 1 [38720/60000 (65%)]\t training loss: 0.428683\n",
            "epoch: 1 [39040/60000 (65%)]\t training loss: 0.046154\n",
            "epoch: 1 [39360/60000 (66%)]\t training loss: 0.152741\n",
            "epoch: 1 [39680/60000 (66%)]\t training loss: 0.021961\n",
            "epoch: 1 [40000/60000 (67%)]\t training loss: 0.145118\n",
            "epoch: 1 [40320/60000 (67%)]\t training loss: 0.064329\n",
            "epoch: 1 [40640/60000 (68%)]\t training loss: 0.170822\n",
            "epoch: 1 [40960/60000 (68%)]\t training loss: 0.208453\n",
            "epoch: 1 [41280/60000 (69%)]\t training loss: 0.186163\n",
            "epoch: 1 [41600/60000 (69%)]\t training loss: 0.165556\n",
            "epoch: 1 [41920/60000 (70%)]\t training loss: 0.042681\n",
            "epoch: 1 [42240/60000 (70%)]\t training loss: 0.008819\n",
            "epoch: 1 [42560/60000 (71%)]\t training loss: 0.050518\n",
            "epoch: 1 [42880/60000 (71%)]\t training loss: 0.068756\n",
            "epoch: 1 [43200/60000 (72%)]\t training loss: 0.050549\n",
            "epoch: 1 [43520/60000 (73%)]\t training loss: 0.226462\n",
            "epoch: 1 [43840/60000 (73%)]\t training loss: 0.047861\n",
            "epoch: 1 [44160/60000 (74%)]\t training loss: 0.112417\n",
            "epoch: 1 [44480/60000 (74%)]\t training loss: 0.057739\n",
            "epoch: 1 [44800/60000 (75%)]\t training loss: 0.255429\n",
            "epoch: 1 [45120/60000 (75%)]\t training loss: 0.031167\n",
            "epoch: 1 [45440/60000 (76%)]\t training loss: 0.177306\n",
            "epoch: 1 [45760/60000 (76%)]\t training loss: 0.004616\n",
            "epoch: 1 [46080/60000 (77%)]\t training loss: 0.244074\n",
            "epoch: 1 [46400/60000 (77%)]\t training loss: 0.043274\n",
            "epoch: 1 [46720/60000 (78%)]\t training loss: 0.078195\n",
            "epoch: 1 [47040/60000 (78%)]\t training loss: 0.025305\n",
            "epoch: 1 [47360/60000 (79%)]\t training loss: 0.025667\n",
            "epoch: 1 [47680/60000 (79%)]\t training loss: 0.235931\n",
            "epoch: 1 [48000/60000 (80%)]\t training loss: 0.200313\n",
            "epoch: 1 [48320/60000 (81%)]\t training loss: 0.173942\n",
            "epoch: 1 [48640/60000 (81%)]\t training loss: 0.015410\n",
            "epoch: 1 [48960/60000 (82%)]\t training loss: 0.064077\n",
            "epoch: 1 [49280/60000 (82%)]\t training loss: 0.186568\n",
            "epoch: 1 [49600/60000 (83%)]\t training loss: 0.249521\n",
            "epoch: 1 [49920/60000 (83%)]\t training loss: 0.114099\n",
            "epoch: 1 [50240/60000 (84%)]\t training loss: 0.061973\n",
            "epoch: 1 [50560/60000 (84%)]\t training loss: 0.002961\n",
            "epoch: 1 [50880/60000 (85%)]\t training loss: 0.004606\n",
            "epoch: 1 [51200/60000 (85%)]\t training loss: 0.247760\n",
            "epoch: 1 [51520/60000 (86%)]\t training loss: 0.026552\n",
            "epoch: 1 [51840/60000 (86%)]\t training loss: 0.007000\n",
            "epoch: 1 [52160/60000 (87%)]\t training loss: 0.008431\n",
            "epoch: 1 [52480/60000 (87%)]\t training loss: 0.009096\n",
            "epoch: 1 [52800/60000 (88%)]\t training loss: 0.033623\n",
            "epoch: 1 [53120/60000 (89%)]\t training loss: 0.036568\n",
            "epoch: 1 [53440/60000 (89%)]\t training loss: 0.023789\n",
            "epoch: 1 [53760/60000 (90%)]\t training loss: 0.027706\n",
            "epoch: 1 [54080/60000 (90%)]\t training loss: 0.004158\n",
            "epoch: 1 [54400/60000 (91%)]\t training loss: 0.148534\n",
            "epoch: 1 [54720/60000 (91%)]\t training loss: 0.010018\n",
            "epoch: 1 [55040/60000 (92%)]\t training loss: 0.001289\n",
            "epoch: 1 [55360/60000 (92%)]\t training loss: 0.067191\n",
            "epoch: 1 [55680/60000 (93%)]\t training loss: 0.022575\n",
            "epoch: 1 [56000/60000 (93%)]\t training loss: 0.060700\n",
            "epoch: 1 [56320/60000 (94%)]\t training loss: 0.148430\n",
            "epoch: 1 [56640/60000 (94%)]\t training loss: 0.051501\n",
            "epoch: 1 [56960/60000 (95%)]\t training loss: 0.027410\n",
            "epoch: 1 [57280/60000 (95%)]\t training loss: 0.069873\n",
            "epoch: 1 [57600/60000 (96%)]\t training loss: 0.002970\n",
            "epoch: 1 [57920/60000 (97%)]\t training loss: 0.078287\n",
            "epoch: 1 [58240/60000 (97%)]\t training loss: 0.017298\n",
            "epoch: 1 [58560/60000 (98%)]\t training loss: 0.007776\n",
            "epoch: 1 [58880/60000 (98%)]\t training loss: 0.075148\n",
            "epoch: 1 [59200/60000 (99%)]\t training loss: 0.004164\n",
            "epoch: 1 [59520/60000 (99%)]\t training loss: 0.032162\n",
            "epoch: 1 [59840/60000 (100%)]\t training loss: 0.026846\n",
            "\n",
            "Test dataset: Overall Loss: 0.0479, Overall Accuracy: 9837/10000 (98%)\n",
            "\n",
            "epoch: 2 [0/60000 (0%)]\t training loss: 0.032459\n",
            "epoch: 2 [320/60000 (1%)]\t training loss: 0.051872\n",
            "epoch: 2 [640/60000 (1%)]\t training loss: 0.021002\n",
            "epoch: 2 [960/60000 (2%)]\t training loss: 0.161845\n",
            "epoch: 2 [1280/60000 (2%)]\t training loss: 0.011314\n",
            "epoch: 2 [1600/60000 (3%)]\t training loss: 0.183486\n",
            "epoch: 2 [1920/60000 (3%)]\t training loss: 0.255296\n",
            "epoch: 2 [2240/60000 (4%)]\t training loss: 0.003424\n",
            "epoch: 2 [2560/60000 (4%)]\t training loss: 0.168998\n",
            "epoch: 2 [2880/60000 (5%)]\t training loss: 0.228925\n",
            "epoch: 2 [3200/60000 (5%)]\t training loss: 0.053332\n",
            "epoch: 2 [3520/60000 (6%)]\t training loss: 0.007549\n",
            "epoch: 2 [3840/60000 (6%)]\t training loss: 0.003140\n",
            "epoch: 2 [4160/60000 (7%)]\t training loss: 0.021184\n",
            "epoch: 2 [4480/60000 (7%)]\t training loss: 0.005962\n",
            "epoch: 2 [4800/60000 (8%)]\t training loss: 0.044938\n",
            "epoch: 2 [5120/60000 (9%)]\t training loss: 0.048680\n",
            "epoch: 2 [5440/60000 (9%)]\t training loss: 0.036868\n",
            "epoch: 2 [5760/60000 (10%)]\t training loss: 0.045095\n",
            "epoch: 2 [6080/60000 (10%)]\t training loss: 0.004067\n",
            "epoch: 2 [6400/60000 (11%)]\t training loss: 0.030604\n",
            "epoch: 2 [6720/60000 (11%)]\t training loss: 0.018710\n",
            "epoch: 2 [7040/60000 (12%)]\t training loss: 0.074289\n",
            "epoch: 2 [7360/60000 (12%)]\t training loss: 0.011467\n",
            "epoch: 2 [7680/60000 (13%)]\t training loss: 0.009166\n",
            "epoch: 2 [8000/60000 (13%)]\t training loss: 0.142967\n",
            "epoch: 2 [8320/60000 (14%)]\t training loss: 0.252711\n",
            "epoch: 2 [8640/60000 (14%)]\t training loss: 0.085751\n",
            "epoch: 2 [8960/60000 (15%)]\t training loss: 0.107989\n",
            "epoch: 2 [9280/60000 (15%)]\t training loss: 0.159306\n",
            "epoch: 2 [9600/60000 (16%)]\t training loss: 0.012876\n",
            "epoch: 2 [9920/60000 (17%)]\t training loss: 0.289011\n",
            "epoch: 2 [10240/60000 (17%)]\t training loss: 0.004284\n",
            "epoch: 2 [10560/60000 (18%)]\t training loss: 0.042315\n",
            "epoch: 2 [10880/60000 (18%)]\t training loss: 0.193544\n",
            "epoch: 2 [11200/60000 (19%)]\t training loss: 0.018717\n",
            "epoch: 2 [11520/60000 (19%)]\t training loss: 0.010678\n",
            "epoch: 2 [11840/60000 (20%)]\t training loss: 0.008473\n",
            "epoch: 2 [12160/60000 (20%)]\t training loss: 0.013764\n",
            "epoch: 2 [12480/60000 (21%)]\t training loss: 0.170687\n",
            "epoch: 2 [12800/60000 (21%)]\t training loss: 0.022820\n",
            "epoch: 2 [13120/60000 (22%)]\t training loss: 0.003888\n",
            "epoch: 2 [13440/60000 (22%)]\t training loss: 0.001844\n",
            "epoch: 2 [13760/60000 (23%)]\t training loss: 0.043468\n",
            "epoch: 2 [14080/60000 (23%)]\t training loss: 0.028132\n",
            "epoch: 2 [14400/60000 (24%)]\t training loss: 0.072534\n",
            "epoch: 2 [14720/60000 (25%)]\t training loss: 0.008577\n",
            "epoch: 2 [15040/60000 (25%)]\t training loss: 0.003882\n",
            "epoch: 2 [15360/60000 (26%)]\t training loss: 0.032451\n",
            "epoch: 2 [15680/60000 (26%)]\t training loss: 0.130089\n",
            "epoch: 2 [16000/60000 (27%)]\t training loss: 0.080569\n",
            "epoch: 2 [16320/60000 (27%)]\t training loss: 0.218174\n",
            "epoch: 2 [16640/60000 (28%)]\t training loss: 0.066747\n",
            "epoch: 2 [16960/60000 (28%)]\t training loss: 0.032819\n",
            "epoch: 2 [17280/60000 (29%)]\t training loss: 0.006708\n",
            "epoch: 2 [17600/60000 (29%)]\t training loss: 0.013914\n",
            "epoch: 2 [17920/60000 (30%)]\t training loss: 0.156385\n",
            "epoch: 2 [18240/60000 (30%)]\t training loss: 0.094040\n",
            "epoch: 2 [18560/60000 (31%)]\t training loss: 0.061555\n",
            "epoch: 2 [18880/60000 (31%)]\t training loss: 0.021348\n",
            "epoch: 2 [19200/60000 (32%)]\t training loss: 0.042026\n",
            "epoch: 2 [19520/60000 (33%)]\t training loss: 0.108637\n",
            "epoch: 2 [19840/60000 (33%)]\t training loss: 0.010540\n",
            "epoch: 2 [20160/60000 (34%)]\t training loss: 0.009775\n",
            "epoch: 2 [20480/60000 (34%)]\t training loss: 0.004604\n",
            "epoch: 2 [20800/60000 (35%)]\t training loss: 0.057296\n",
            "epoch: 2 [21120/60000 (35%)]\t training loss: 0.036896\n",
            "epoch: 2 [21440/60000 (36%)]\t training loss: 0.028545\n",
            "epoch: 2 [21760/60000 (36%)]\t training loss: 0.118109\n",
            "epoch: 2 [22080/60000 (37%)]\t training loss: 0.034897\n",
            "epoch: 2 [22400/60000 (37%)]\t training loss: 0.315852\n",
            "epoch: 2 [22720/60000 (38%)]\t training loss: 0.061328\n",
            "epoch: 2 [23040/60000 (38%)]\t training loss: 0.031042\n",
            "epoch: 2 [23360/60000 (39%)]\t training loss: 0.010680\n",
            "epoch: 2 [23680/60000 (39%)]\t training loss: 0.078207\n",
            "epoch: 2 [24000/60000 (40%)]\t training loss: 0.023730\n",
            "epoch: 2 [24320/60000 (41%)]\t training loss: 0.579066\n",
            "epoch: 2 [24640/60000 (41%)]\t training loss: 0.006908\n",
            "epoch: 2 [24960/60000 (42%)]\t training loss: 0.028752\n",
            "epoch: 2 [25280/60000 (42%)]\t training loss: 0.054413\n",
            "epoch: 2 [25600/60000 (43%)]\t training loss: 0.012691\n",
            "epoch: 2 [25920/60000 (43%)]\t training loss: 0.094149\n",
            "epoch: 2 [26240/60000 (44%)]\t training loss: 0.015563\n",
            "epoch: 2 [26560/60000 (44%)]\t training loss: 0.002888\n",
            "epoch: 2 [26880/60000 (45%)]\t training loss: 0.012627\n",
            "epoch: 2 [27200/60000 (45%)]\t training loss: 0.018161\n",
            "epoch: 2 [27520/60000 (46%)]\t training loss: 0.124478\n",
            "epoch: 2 [27840/60000 (46%)]\t training loss: 0.008674\n",
            "epoch: 2 [28160/60000 (47%)]\t training loss: 0.001095\n",
            "epoch: 2 [28480/60000 (47%)]\t training loss: 0.189419\n",
            "epoch: 2 [28800/60000 (48%)]\t training loss: 0.207304\n",
            "epoch: 2 [29120/60000 (49%)]\t training loss: 0.070176\n",
            "epoch: 2 [29440/60000 (49%)]\t training loss: 0.004605\n",
            "epoch: 2 [29760/60000 (50%)]\t training loss: 0.020699\n",
            "epoch: 2 [30080/60000 (50%)]\t training loss: 0.224005\n",
            "epoch: 2 [30400/60000 (51%)]\t training loss: 0.192923\n",
            "epoch: 2 [30720/60000 (51%)]\t training loss: 0.131076\n",
            "epoch: 2 [31040/60000 (52%)]\t training loss: 0.008489\n",
            "epoch: 2 [31360/60000 (52%)]\t training loss: 0.022954\n",
            "epoch: 2 [31680/60000 (53%)]\t training loss: 0.032922\n",
            "epoch: 2 [32000/60000 (53%)]\t training loss: 0.059704\n",
            "epoch: 2 [32320/60000 (54%)]\t training loss: 0.027170\n",
            "epoch: 2 [32640/60000 (54%)]\t training loss: 0.050589\n",
            "epoch: 2 [32960/60000 (55%)]\t training loss: 0.028838\n",
            "epoch: 2 [33280/60000 (55%)]\t training loss: 0.095994\n",
            "epoch: 2 [33600/60000 (56%)]\t training loss: 0.018744\n",
            "epoch: 2 [33920/60000 (57%)]\t training loss: 0.110716\n",
            "epoch: 2 [34240/60000 (57%)]\t training loss: 0.161577\n",
            "epoch: 2 [34560/60000 (58%)]\t training loss: 0.250419\n",
            "epoch: 2 [34880/60000 (58%)]\t training loss: 0.027281\n",
            "epoch: 2 [35200/60000 (59%)]\t training loss: 0.044553\n",
            "epoch: 2 [35520/60000 (59%)]\t training loss: 0.022255\n",
            "epoch: 2 [35840/60000 (60%)]\t training loss: 0.008434\n",
            "epoch: 2 [36160/60000 (60%)]\t training loss: 0.036915\n",
            "epoch: 2 [36480/60000 (61%)]\t training loss: 0.029273\n",
            "epoch: 2 [36800/60000 (61%)]\t training loss: 0.004734\n",
            "epoch: 2 [37120/60000 (62%)]\t training loss: 0.009614\n",
            "epoch: 2 [37440/60000 (62%)]\t training loss: 0.061293\n",
            "epoch: 2 [37760/60000 (63%)]\t training loss: 0.312273\n",
            "epoch: 2 [38080/60000 (63%)]\t training loss: 0.145517\n",
            "epoch: 2 [38400/60000 (64%)]\t training loss: 0.026672\n",
            "epoch: 2 [38720/60000 (65%)]\t training loss: 0.057358\n",
            "epoch: 2 [39040/60000 (65%)]\t training loss: 0.008203\n",
            "epoch: 2 [39360/60000 (66%)]\t training loss: 0.023614\n",
            "epoch: 2 [39680/60000 (66%)]\t training loss: 0.148070\n",
            "epoch: 2 [40000/60000 (67%)]\t training loss: 0.071601\n",
            "epoch: 2 [40320/60000 (67%)]\t training loss: 0.033377\n",
            "epoch: 2 [40640/60000 (68%)]\t training loss: 0.017742\n",
            "epoch: 2 [40960/60000 (68%)]\t training loss: 0.042294\n",
            "epoch: 2 [41280/60000 (69%)]\t training loss: 0.082278\n",
            "epoch: 2 [41600/60000 (69%)]\t training loss: 0.016018\n",
            "epoch: 2 [41920/60000 (70%)]\t training loss: 0.001428\n",
            "epoch: 2 [42240/60000 (70%)]\t training loss: 0.001583\n",
            "epoch: 2 [42560/60000 (71%)]\t training loss: 0.036605\n",
            "epoch: 2 [42880/60000 (71%)]\t training loss: 0.047737\n",
            "epoch: 2 [43200/60000 (72%)]\t training loss: 0.002310\n",
            "epoch: 2 [43520/60000 (73%)]\t training loss: 0.079524\n",
            "epoch: 2 [43840/60000 (73%)]\t training loss: 0.075941\n",
            "epoch: 2 [44160/60000 (74%)]\t training loss: 0.134918\n",
            "epoch: 2 [44480/60000 (74%)]\t training loss: 0.001669\n",
            "epoch: 2 [44800/60000 (75%)]\t training loss: 0.320092\n",
            "epoch: 2 [45120/60000 (75%)]\t training loss: 0.038301\n",
            "epoch: 2 [45440/60000 (76%)]\t training loss: 0.321082\n",
            "epoch: 2 [45760/60000 (76%)]\t training loss: 0.012810\n",
            "epoch: 2 [46080/60000 (77%)]\t training loss: 0.011047\n",
            "epoch: 2 [46400/60000 (77%)]\t training loss: 0.028959\n",
            "epoch: 2 [46720/60000 (78%)]\t training loss: 0.001253\n",
            "epoch: 2 [47040/60000 (78%)]\t training loss: 0.083520\n",
            "epoch: 2 [47360/60000 (79%)]\t training loss: 0.154127\n",
            "epoch: 2 [47680/60000 (79%)]\t training loss: 0.064245\n",
            "epoch: 2 [48000/60000 (80%)]\t training loss: 0.029277\n",
            "epoch: 2 [48320/60000 (81%)]\t training loss: 0.077967\n",
            "epoch: 2 [48640/60000 (81%)]\t training loss: 0.001426\n",
            "epoch: 2 [48960/60000 (82%)]\t training loss: 0.008259\n",
            "epoch: 2 [49280/60000 (82%)]\t training loss: 0.017825\n",
            "epoch: 2 [49600/60000 (83%)]\t training loss: 0.017213\n",
            "epoch: 2 [49920/60000 (83%)]\t training loss: 0.383439\n",
            "epoch: 2 [50240/60000 (84%)]\t training loss: 0.150853\n",
            "epoch: 2 [50560/60000 (84%)]\t training loss: 0.093228\n",
            "epoch: 2 [50880/60000 (85%)]\t training loss: 0.067694\n",
            "epoch: 2 [51200/60000 (85%)]\t training loss: 0.073526\n",
            "epoch: 2 [51520/60000 (86%)]\t training loss: 0.050110\n",
            "epoch: 2 [51840/60000 (86%)]\t training loss: 0.011362\n",
            "epoch: 2 [52160/60000 (87%)]\t training loss: 0.032216\n",
            "epoch: 2 [52480/60000 (87%)]\t training loss: 0.005757\n",
            "epoch: 2 [52800/60000 (88%)]\t training loss: 0.002141\n",
            "epoch: 2 [53120/60000 (89%)]\t training loss: 0.025421\n",
            "epoch: 2 [53440/60000 (89%)]\t training loss: 0.063312\n",
            "epoch: 2 [53760/60000 (90%)]\t training loss: 0.041098\n",
            "epoch: 2 [54080/60000 (90%)]\t training loss: 0.048403\n",
            "epoch: 2 [54400/60000 (91%)]\t training loss: 0.005818\n",
            "epoch: 2 [54720/60000 (91%)]\t training loss: 0.146720\n",
            "epoch: 2 [55040/60000 (92%)]\t training loss: 0.155649\n",
            "epoch: 2 [55360/60000 (92%)]\t training loss: 0.027811\n",
            "epoch: 2 [55680/60000 (93%)]\t training loss: 0.006175\n",
            "epoch: 2 [56000/60000 (93%)]\t training loss: 0.062857\n",
            "epoch: 2 [56320/60000 (94%)]\t training loss: 0.006423\n",
            "epoch: 2 [56640/60000 (94%)]\t training loss: 0.001272\n",
            "epoch: 2 [56960/60000 (95%)]\t training loss: 0.109403\n",
            "epoch: 2 [57280/60000 (95%)]\t training loss: 0.162840\n",
            "epoch: 2 [57600/60000 (96%)]\t training loss: 0.325476\n",
            "epoch: 2 [57920/60000 (97%)]\t training loss: 0.129670\n",
            "epoch: 2 [58240/60000 (97%)]\t training loss: 0.072420\n",
            "epoch: 2 [58560/60000 (98%)]\t training loss: 0.006667\n",
            "epoch: 2 [58880/60000 (98%)]\t training loss: 0.011987\n",
            "epoch: 2 [59200/60000 (99%)]\t training loss: 0.004224\n",
            "epoch: 2 [59520/60000 (99%)]\t training loss: 0.012263\n",
            "epoch: 2 [59840/60000 (100%)]\t training loss: 0.030646\n",
            "\n",
            "Test dataset: Overall Loss: 0.0386, Overall Accuracy: 9871/10000 (99%)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_samples = enumerate(test_dataloader)\n",
        "b_i, (sample_data, sample_targets) = next(test_samples)\n",
        "\n",
        "plt.imshow(sample_data[0][0], cmap='gray', interpolation='none')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 430
        },
        "id": "8GJTMjyaiY0-",
        "outputId": "2a36fe51-57fe-403c-cf9b-e3136ed034eb"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Model prediction is : {model(sample_data).data.max(1)[1][0]}\")\n",
        "print(f\"Ground truth is : {sample_targets[0]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m2IvQcb-ia_P",
        "outputId": "cd2d0032-af91-44a2-b7e3-b16d027ef4ca"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model prediction is : 0\n",
            "Ground truth is : 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "모델의 매개변수만 저장"
      ],
      "metadata": {
        "id": "cwztMvu5igPX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "PATH_TO_MODEL = \"./convnet.pth\"\n",
        "torch.save(model.state_dict(), PATH_TO_MODEL)"
      ],
      "metadata": {
        "id": "07SF4ZhSicKu"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model Load"
      ],
      "metadata": {
        "id": "kcB5-30Li9eu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from PIL import Image"
      ],
      "metadata": {
        "id": "2zMELb4ZijEH"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ConvNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ConvNet, self).__init__()\n",
        "        self.cn1 = nn.Conv2d(1, 16, 3, 1)\n",
        "        self.cn2 = nn.Conv2d(16, 32, 3, 1)\n",
        "        self.dp1 = nn.Dropout2d(0.10)\n",
        "        self.dp2 = nn.Dropout2d(0.25)\n",
        "        self.fc1 = nn.Linear(4608, 64) # 4608 is basically 12 X 12 X 32\n",
        "        self.fc2 = nn.Linear(64, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.cn1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.cn2(x)\n",
        "        x = F.relu(x)\n",
        "        x = F.max_pool2d(x, 2)\n",
        "        x = self.dp1(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.fc1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.dp2(x)\n",
        "        x = self.fc2(x)\n",
        "        op = F.log_softmax(x, dim=1)\n",
        "        return op\n",
        "\n",
        "model = ConvNet()"
      ],
      "metadata": {
        "id": "z0tMLmS9jBGp"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "모델 정의는 원래 python script로 `cnn_model.py` 로 저장되고,  \n",
        "```python\n",
        "from cnn_model import ConvNet\n",
        "model = ConvNet()\n",
        "```\n",
        "으로 작성된다.\n",
        "\n",
        "이 실습의 경우 Jupyter로 작업하므로 모델 정의를 다시 쓰고 인스턴스화한다."
      ],
      "metadata": {
        "id": "eBUN4PS-jGtP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "PATH_TO_MODEL = \"./convnet.pth\"\n",
        "model.load_state_dict(torch.load(PATH_TO_MODEL, map_location=\"cpu\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H7swsP3XjWjT",
        "outputId": "0bb6b4f9-b690-48dc-be38-dfeda91e8764"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "`<All keys matched successfully>` : 매개변수 로딩에 성공. 즉, 우리가 인스턴스화한 모델의 매개변수가 저장한 모델과 동일한 구조를 갖는다는 뜻."
      ],
      "metadata": {
        "id": "g1t3xQfWjcm7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U8vmkOahjZdv",
        "outputId": "5bd78c92-70c5-459c-8731-d4541415fc34"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ConvNet(\n",
              "  (cn1): Conv2d(1, 16, kernel_size=(3, 3), stride=(1, 1))\n",
              "  (cn2): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1))\n",
              "  (dp1): Dropout2d(p=0.1, inplace=False)\n",
              "  (dp2): Dropout2d(p=0.25, inplace=False)\n",
              "  (fc1): Linear(in_features=4608, out_features=64, bias=True)\n",
              "  (fc2): Linear(in_features=64, out_features=10, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "로딩된 모델의 매개변수 값을 업데이트 또는 변경하지 않게 evaluation 모드로 지정"
      ],
      "metadata": {
        "id": "EYmij416jnof"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "image = Image.open(\"./digit_image.jpg\")"
      ],
      "metadata": {
        "id": "iOabPPJHjnTf"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "qDlhW-PBjsmH",
        "outputId": "93b277be-44a2-48d5-8b96-10938fe28596"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<PIL.PngImagePlugin.PngImageFile image mode=RGBA size=251x248>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD4CAYAAAAq5pAIAAAOGUlEQVR4nO3db6xU9Z3H8c93saBCNSBCbqzZ26IPrGukhpgma9S1tkHBIFEbeLBBRS8PEGtsbAk+qGatMbvblUcSaTS9axDEKBaxhrKAtWtCFYmLqEsRg+Fe+aPLAy4miMB3H8xh96r3/M71zJk54/2+X8nNzJzvnDnfTPhwzpzfzPmZuwvAyPc3dTcAoD0IOxAEYQeCIOxAEIQdCOK0dm7MzDj1D7SYu9tQy5vas5vZdDPbaWbvm9niZl4LQGtZ2XF2Mxsl6a+SfiypT9Ibkua6+7uJddizAy3Wij375ZLed/cP3P2YpFWSZjXxegBaqJmwnydp76DHfdmyLzCzHjPbamZbm9gWgCa1/ASduy+XtFziMB6oUzN79n5J5w96/J1sGYAO1EzY35B0oZl918xGS5ojaW01bQGoWunDeHc/bmZ3SVovaZSkJ939nco6A1Cp0kNvpTbGZ3ag5VrypRoA3xyEHQiCsANBEHYgCMIOBEHYgSAIOxAEYQeCIOxAEIQdCIKwA0EQdiAIwg4E0dZLSaM1xo8fn1u75pprkuted911yfott9ySrG/ZsqV0ffv27cl1P/vss2R93bp1yTq+iD07EARhB4Ig7EAQhB0IgrADQRB2IAjCDgTB1WVHgGnTpuXWXn/99eS6mzZtamrbReP4rfTSSy8l67fffntu7eOPP666nY7B1WWB4Ag7EARhB4Ig7EAQhB0IgrADQRB2IAjG2UeAiRMn5tZmzJiRXPeFF15I1gcGBpL10aNHJ+tjx47Nrc2cOTO57pgxY5L1RYsWJevd3d25tZtvvjm57vr165P1TpY3zt7UxSvMbI+kAUknJB139/xvdwCoVRVXqvkHd/+kgtcB0EJ8ZgeCaDbsLumPZvammfUM9QQz6zGzrWa2tcltAWhCs4fxV7h7v5lNkrTBzP7b3V8d/AR3Xy5pucQJOqBOTe3Z3b0/uz0oaY2ky6toCkD1SofdzMaa2bdP3Zf0E0k7qmoMQLVKj7Ob2ffU2JtLjY8DT7v7rwvW4TAelUmN4UvS2rVrc2uTJk1KrnvJJZeU6qkTVD7O7u4fSLq0dEcA2oqhNyAIwg4EQdiBIAg7EARhB4LgJ64YsaZOnZpb27ZtW3Ld2267LVnv7e0t01JbcClpIDjCDgRB2IEgCDsQBGEHgiDsQBCEHQiiigtOAh3p8OHDpde99tprk/VOHmfPw54dCIKwA0EQdiAIwg4EQdiBIAg7EARhB4JgnB3fWKedlv7n++CDD+bWiq7j8Morr5RpqaOxZweCIOxAEIQdCIKwA0EQdiAIwg4EQdiBILhuPL6xbrrppmT92Wefza0dPXo0ue6ZZ55ZqqdOUPq68Wb2pJkdNLMdg5ZNMLMNZrYrux1fZbMAqjecw/jfSZr+pWWLJW109wslbcweA+hghWF391clHfrS4lmSTl2Xp1fSjdW2BaBqZb8bP9nd92X390uanPdEM+uR1FNyOwAq0vQPYdzdUyfe3H25pOUSJ+iAOpUdejtgZl2SlN0erK4lAK1QNuxrJc3L7s+T9Ptq2gHQKoWH8Wa2UtLVkiaaWZ+kX0l6RNJqM5sv6UNJP21lk4jpggsuSNafeuqpZP3kyZO5tSVLlpTq6ZusMOzuPjen9KOKewHQQnxdFgiCsANBEHYgCMIOBEHYgSC4lDRqc9ZZZyXra9asSdZHjx6drN933325taVLlybXHYnYswNBEHYgCMIOBEHYgSAIOxAEYQeCIOxAEIyzjwCnn356bu2GG25Irjt//vxkfe/evcn6smXLkvXdu3fn1h5//PHkuhdffHGy/vLLLyfrjz76aLIeDXt2IAjCDgRB2IEgCDsQBGEHgiDsQBCEHQiCKZs7wLnnnpusX3XVVcn6/fffn1u79NJLS/U0XMeOHUvWU+PsF110UXLdojH+onH4I0eOJOsjVekpmwGMDIQdCIKwA0EQdiAIwg4EQdiBIAg7EATj7BU455xzkvV77703Wb/jjjuS9aJx+JT+/v5kfeHChcn60aNHk/VnnnkmWT/77LNza59++mly3aLvF2zbti1Zj6r0OLuZPWlmB81sx6BlD5hZv5m9lf1dX2WzAKo3nMP430maPsTyR919avb3h2rbAlC1wrC7+6uSDrWhFwAt1MwJurvMbHt2mD8+70lm1mNmW81saxPbAtCksmFfJmmKpKmS9kn6Td4T3X25u09z92kltwWgAqXC7u4H3P2Eu5+U9FtJl1fbFoCqlQq7mXUNejhb0o685wLoDIXXjTezlZKuljTRzPok/UrS1WY2VZJL2iNpQeta7Azd3d25tddeey25bldXV7JeZP/+/cn6kiVLcmurVq1KrnvixIlkvWge89Q4epGibR8+fLj0a+OrCsPu7nOHWPxEC3oB0EJ8XRYIgrADQRB2IAjCDgRB2IEg+IlrZsyYMcn66tWrc2tF0yJ//vnnyfpDDz2UrG/YsCFZ37JlS7KeMmfOnGT96aefLv3akrRz587c2pQpU5Lr9vX1JetXXnllU+uPVFxKGgiOsANBEHYgCMIOBEHYgSAIOxAEYQeCKPzVWxTTpw91Tc3/lxpLP378eHLdW2+9NVlfuXJlst6Mou8APPFEcz9g3LVrV7I+c+bM3FrROPnDDz+crBf9vDbqOHse9uxAEIQdCIKwA0EQdiAIwg4EQdiBIAg7EATj7JmiMd+UzZs3J+vNjqOPGjUqWZ8xY0bpbZ9xxhnJ+qZNm5L12bNnJ+sDAwO5td27dyfXXbFiRbI+bty4ZB1fxJ4dCIKwA0EQdiAIwg4EQdiBIAg7EARhB4JgnD0zadKk0uv29/cn60Vj2Zdddlmyfueddybr8+bNy60VzQvQ29ubrN99993JemocvVnHjh1L1g8dOtSybY9EhXt2MzvfzDab2btm9o6Z/SxbPsHMNpjZrux2fOvbBVDWcA7jj0v6ubt/X9IPJS00s+9LWixpo7tfKGlj9hhAhyoMu7vvc/dt2f0BSe9JOk/SLEmnjgF7Jd3Yoh4BVOBrfWY3s25JP5D0F0mT3X1fVtovaXLOOj2SeproEUAFhn023szGSXpO0j3ufnhwzRtngYY8E+Tuy919mrtPa6pTAE0ZVtjN7FtqBH2Fuz+fLT5gZl1ZvUvSwda0CKAKhVM2m5mp8Zn8kLvfM2j5v0j6H3d/xMwWS5rg7r8oeK2OnbJ5wYIFyfpjjz2WW2u8RZ2paGht0aJFyfqRI0eqbAdtkDdl83A+s/+9pH+U9LaZvZUtWyLpEUmrzWy+pA8l/bSCPgG0SGHY3f0/JeXtun5UbTsAWoWvywJBEHYgCMIOBEHYgSAIOxBE4Th7pRvr4HH2Iqmpj5cuXZpct+inmEWXVH7xxReT9Y8++ii3VnSZa4w8eePs7NmBIAg7EARhB4Ig7EAQhB0IgrADQRB2IAjG2YERhnF2IDjCDgRB2IEgCDsQBGEHgiDsQBCEHQiCsANBEHYgCMIOBEHYgSAIOxAEYQeCIOxAEIQdCKIw7GZ2vpltNrN3zewdM/tZtvwBM+s3s7eyv+tb3y6AsgovXmFmXZK63H2bmX1b0puSblRjPvYj7v6vw94YF68AWi7v4hXDmZ99n6R92f0BM3tP0nnVtgeg1b7WZ3Yz65b0A0l/yRbdZWbbzexJMxufs06PmW01s63NtQqgGcO+Bp2ZjZP0J0m/dvfnzWyypE8kuaR/UuNQ//aC1+AwHmixvMP4YYXdzL4laZ2k9e7+b0PUuyWtc/e/K3gdwg60WOkLTpqZSXpC0nuDg56duDtltqQdzTYJoHWGczb+Ckl/lvS2pJPZ4iWS5kqaqsZh/B5JC7KTeanXYs8OtFhTh/FVIexA63HdeCA4wg4EQdiBIAg7EARhB4Ig7EAQhB0IgrADQRB2IAjCDgRB2IEgCDsQBGEHgiDsQBCFF5ys2CeSPhz0eGK2rBN1am+d2pdEb2VV2dvf5hXa+nv2r2zcbKu7T6utgYRO7a1T+5Lorax29cZhPBAEYQeCqDvsy2vefkqn9tapfUn0VlZbeqv1MzuA9ql7zw6gTQg7EEQtYTez6Wa208zeN7PFdfSQx8z2mNnb2TTUtc5Pl82hd9DMdgxaNsHMNpjZrux2yDn2auqtI6bxTkwzXut7V/f0523/zG5moyT9VdKPJfVJekPSXHd/t62N5DCzPZKmuXvtX8AwsyslHZH076em1jKzf5Z0yN0fyf6jHO/uv+yQ3h7Q15zGu0W95U0zfqtqfO+qnP68jDr27JdLet/dP3D3Y5JWSZpVQx8dz91flXToS4tnSerN7veq8Y+l7XJ66wjuvs/dt2X3BySdmma81vcu0Vdb1BH28yTtHfS4T50137tL+qOZvWlmPXU3M4TJg6bZ2i9pcp3NDKFwGu92+tI04x3z3pWZ/rxZnKD7qivc/TJJ10lamB2udiRvfAbrpLHTZZKmqDEH4D5Jv6mzmWya8eck3ePuhwfX6nzvhuirLe9bHWHvl3T+oMffyZZ1BHfvz24PSlqjxseOTnLg1Ay62e3Bmvv5P+5+wN1PuPtJSb9Vje9dNs34c5JWuPvz2eLa37uh+mrX+1ZH2N+QdKGZfdfMRkuaI2ltDX18hZmNzU6cyMzGSvqJOm8q6rWS5mX350n6fY29fEGnTOOdN824an7vap/+3N3b/ifpejXOyO+WdH8dPeT09T1J/5X9vVN3b5JWqnFY97ka5zbmSzpH0kZJuyT9h6QJHdTbU2pM7b1djWB11dTbFWocom+X9Fb2d33d712ir7a8b3xdFgiCE3RAEIQdCIKwA0EQdiAIwg4EQdiBIAg7EMT/ArW6jsy0t+A4AAAAAElFTkSuQmCC\n"
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Preprocessing"
      ],
      "metadata": {
        "id": "j0QkIE-Pj7Kv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def image_to_tensor(image):\n",
        "    gray_image = transforms.functional.to_grayscale(image)\n",
        "    resized_image = transforms.functional.resize(gray_image, (28, 28))\n",
        "    input_image_tensor = transforms.functional.to_tensor(resized_image)\n",
        "    input_image_tensor_norm = transforms.functional.normalize(input_image_tensor, (0.1302,), (0.3069,))\n",
        "    return input_image_tensor_norm"
      ],
      "metadata": {
        "id": "RjgdLqbVj4hO"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model Prediction"
      ],
      "metadata": {
        "id": "O3NGsHwPkAY9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_tensor = image_to_tensor(image)"
      ],
      "metadata": {
        "id": "8d0Cjh-hj9ee"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run_model(input_tensor):\n",
        "    model_input = input_tensor.unsqueeze(0)\n",
        "    with torch.no_grad():\n",
        "        model_output = model(model_input)[0]\n",
        "    model_prediction = model_output.detach().numpy().argmax()\n",
        "    return model_prediction"
      ],
      "metadata": {
        "id": "bI7CTwL_j_yd"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output = run_model(input_tensor)\n",
        "print(output)\n",
        "print(type(output))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mzpLp-VrkMY9",
        "outputId": "301ee993-71aa-4c80-a7b0-5592d7a105c6"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2\n",
            "<class 'numpy.int64'>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py:1345: UserWarning: dropout2d: Received a 2-D input to dropout2d, which is deprecated and will result in an error in a future release. To retain the behavior and silence this warning, please use dropout instead. Note that dropout2d exists to provide channel-wise dropout on inputs with 2 spatial dimensions, a channel dimension, and an optional batch dimension (i.e. 3D or 4D inputs).\n",
            "  warnings.warn(warn_msg)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def debug_model(input_tensor):\n",
        "    model_input = input_tensor.unsqueeze(0)\n",
        "    with torch.no_grad():\n",
        "        model_output = model(model_input)[0]\n",
        "    model_prediction = model_output.detach().numpy()\n",
        "    return np.exp(model_prediction)"
      ],
      "metadata": {
        "id": "TjxfAF7Jkj-c"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "디버깅을 위해 원래 리스트를 변환 후 출력하는 함수"
      ],
      "metadata": {
        "id": "DaOillbPkte0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(debug_model(input_tensor))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fsPjTs_Eko3k",
        "outputId": "a7cf72c6-ff25-4d3d-bf5c-251b97c02417"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[4.9872295e-05 9.5596715e-07 9.9590880e-01 1.3675040e-04 1.5207384e-04\n",
            " 3.4323725e-07 7.9911570e-06 4.4522176e-06 3.7299104e-03 8.7669405e-06]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Post Processing"
      ],
      "metadata": {
        "id": "D_eXfyS6k0rE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "음성 인식 같은 경우 smoothing, outlier 제거 등을 통해 출력 파형을 처리해야 할 것이다."
      ],
      "metadata": {
        "id": "x_xtCeNVk3XP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def post_process(output):\n",
        "    return str(output)"
      ],
      "metadata": {
        "id": "VbGDyS1ckp08"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_output = post_process(output)\n",
        "print(final_output)\n",
        "print(type(final_output))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aZZjfA8BlCTJ",
        "outputId": "0ea66199-4075-42d2-8925-3c0c874cc4bb"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2\n",
            "<class 'str'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Server"
      ],
      "metadata": {
        "id": "T35PgVySlgsy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "server.py  \n",
        "make_request.py"
      ],
      "metadata": {
        "id": "eKizw1jSsuZ8"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MfQT-d7Csvux"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}