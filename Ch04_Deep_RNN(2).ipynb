{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNf/OMElX72iyjvN4zOZmz7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/All4Nothing/pytorch-DL-project/blob/main/Ch04_Deep_RNN(2).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 04. 심층 순환 신경망 아키텍처 (Deep RNN Architecture)"
      ],
      "metadata": {
        "id": "GGawXE5ixMzF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 양방향 LSTM 만들기"
      ],
      "metadata": {
        "id": "7z5upDZcxN0U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "LSTM은 시간 단계상 몇 단계 전이라도 중요한 정보는 보존하고 최근 정보라도 관련 없는 정보는 망각하는 데 도움이 되는 메모리 셀 게이트 덕분에 더 긴 시퀸스를 더 잘 처리할 수 있다. 경사가 폭발하거나 소실하는 문제를 확인하고 긴 영화 리뷰를 처리할 때 LSTM의 성능이 더 좋다.  \n",
        "또한, 모델이 영화 리뷰의 감성에 대해 좀 더 정보에 입각한 결정을 내릴 수 있게 언제든지 컨텍스트 윈도를 확장할 수 있도록 양방향 모델을 사용할 것이다.  \n",
        "또, 과적합을 해결하기 위해 LSTM 모델에 regularization 방법으로 드롭아웃을 사용하겠다."
      ],
      "metadata": {
        "id": "EZUr8IydxQS8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import time\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from string import punctuation\n",
        "from collections import Counter\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "torch.manual_seed(123)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xChBsFfexMCG",
        "outputId": "82c071de-b0f5-4b33-c24c-bb1804a66bb2"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7f94bdbf4170>"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchtext==0.6.0 # torchtext error 해결 위해"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v2H7H9fYzH8h",
        "outputId": "4a21bf66-0ee5-4d96-e594-b44447e79078"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torchtext==0.6.0 in /usr/local/lib/python3.10/dist-packages (0.6.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torchtext==0.6.0) (4.66.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchtext==0.6.0) (2.31.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from torchtext==0.6.0) (2.0.1+cu118)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchtext==0.6.0) (1.23.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from torchtext==0.6.0) (1.16.0)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from torchtext==0.6.0) (0.1.99)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext==0.6.0) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext==0.6.0) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext==0.6.0) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext==0.6.0) (2023.7.22)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->torchtext==0.6.0) (3.12.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->torchtext==0.6.0) (4.7.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->torchtext==0.6.0) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->torchtext==0.6.0) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->torchtext==0.6.0) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch->torchtext==0.6.0) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->torchtext==0.6.0) (3.27.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->torchtext==0.6.0) (16.0.6)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->torchtext==0.6.0) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->torchtext==0.6.0) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "zaCzsacqw1P6"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "from torchtext import (data, datasets) # 최신 버전의 torchtext -> from torchtext.legacy import (data, datasets)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "TEXT_FIELD = data.Field(tokenize = data.get_tokenizer(\"basic_english\"), include_lengths = True)\n",
        "LABEL_FIELD = data.LabelField(dtype = torch.float)\n",
        "\n",
        "train_dataset, test_dataset = datasets.IMDB.splits(TEXT_FIELD, LABEL_FIELD)\n",
        "train_dataset, valid_dataset = train_dataset.split(random_state = random.seed(123))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-QwBVCgXxtxK",
        "outputId": "71aa8d75-7efe-4160-b970-7913d60ca6ac"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "downloading aclImdb_v1.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "aclImdb_v1.tar.gz: 100%|██████████| 84.1M/84.1M [00:08<00:00, 9.48MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_VOCABULARY_SIZE = 25000\n",
        "\n",
        "TEXT_FIELD.build_vocab(train_dataset,\n",
        "                 max_size = MAX_VOCABULARY_SIZE)\n",
        "\n",
        "LABEL_FIELD.build_vocab(train_dataset)"
      ],
      "metadata": {
        "id": "lPWcro2YxwHi"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "B_SIZE = 64\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "train_data_iterator, valid_data_iterator, test_data_iterator = data.BucketIterator.splits(\n",
        "    (train_dataset, valid_dataset, test_dataset),\n",
        "    batch_size = B_SIZE,\n",
        "    sort_within_batch = True,\n",
        "    device = device)"
      ],
      "metadata": {
        "id": "m1qUj31mzfaY"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## If you are training using GPUs, we need to use the following function for the pack_padded_sequence method to work\n",
        "## (reference : https://discuss.pytorch.org/t/error-with-lengths-in-pack-padded-sequence/35517/3)\n",
        "if torch.cuda.is_available():\n",
        "    torch.set_default_tensor_type(torch.cuda.FloatTensor)\n",
        "from torch.nn.utils.rnn import pack_padded_sequence, PackedSequence\n",
        "\n",
        "def cuda_pack_padded_sequence(input, lengths, batch_first=False, enforce_sorted=True):\n",
        "    lengths = torch.as_tensor(lengths, dtype=torch.int64)\n",
        "    lengths = lengths.cpu()\n",
        "    if enforce_sorted:\n",
        "        sorted_indices = None\n",
        "    else:\n",
        "        lengths, sorted_indices = torch.sort(lengths, descending=True)\n",
        "        sorted_indices = sorted_indices.to(input.device)\n",
        "        batch_dim = 0 if batch_first else 1\n",
        "        input = input.index_select(batch_dim, sorted_indices)\n",
        "\n",
        "    data, batch_sizes = \\\n",
        "    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)\n",
        "    return PackedSequence(data, batch_sizes, sorted_indices)"
      ],
      "metadata": {
        "id": "iwSLfMDFzhDI"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LSTM(nn.Module):\n",
        "    def __init__(self, vocabulary_size, embedding_dimension, hidden_dimension, output_dimension, dropout, pad_index):\n",
        "        super().__init__()\n",
        "        self.embedding_layer = nn.Embedding(vocabulary_size, embedding_dimension, padding_idx = pad_index)\n",
        "        self.lstm_layer = nn.LSTM(embedding_dimension,\n",
        "                           hidden_dimension,\n",
        "                           num_layers=1,\n",
        "                           bidirectional=True,\n",
        "                           dropout=dropout)\n",
        "        self.fc_layer = nn.Linear(hidden_dimension * 2, output_dimension)\n",
        "        self.dropout_layer = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, sequence, sequence_lengths=None):\n",
        "        if sequence_lengths is None:\n",
        "            sequence_lengths = torch.LongTensor([len(sequence)])\n",
        "\n",
        "        # sequence := (sequence_length, batch_size)\n",
        "        embedded_output = self.dropout_layer(self.embedding_layer(sequence))\n",
        "\n",
        "\n",
        "        # embedded_output := (sequence_length, batch_size, embedding_dimension)\n",
        "        if torch.cuda.is_available():\n",
        "            packed_embedded_output = cuda_pack_padded_sequence(embedded_output, sequence_lengths)\n",
        "        else:\n",
        "            packed_embedded_output = nn.utils.rnn.pack_padded_sequence(embedded_output, sequence_lengths)\n",
        "\n",
        "        packed_output, (hidden_state, cell_state) = self.lstm_layer(packed_embedded_output)\n",
        "        # hidden_state := (num_layers * num_directions, batch_size, hidden_dimension)\n",
        "        # cell_state := (num_layers * num_directions, batch_size, hidden_dimension)\n",
        "\n",
        "        op, op_lengths = nn.utils.rnn.pad_packed_sequence(packed_output)\n",
        "        # op := (sequence_length, batch_size, hidden_dimension * num_directions)\n",
        "\n",
        "        hidden_output = torch.cat((hidden_state[-2,:,:], hidden_state[-1,:,:]), dim = 1)\n",
        "        # hidden_output := (batch_size, hidden_dimension * num_directions)\n",
        "\n",
        "        return self.fc_layer(hidden_output)\n",
        "\n",
        "\n",
        "INPUT_DIMENSION = len(TEXT_FIELD.vocab)\n",
        "EMBEDDING_DIMENSION = 100\n",
        "HIDDEN_DIMENSION = 32\n",
        "OUTPUT_DIMENSION = 1\n",
        "DROPOUT = 0.5\n",
        "PAD_INDEX = TEXT_FIELD.vocab.stoi[TEXT_FIELD.pad_token]\n",
        "\n",
        "lstm_model = LSTM(INPUT_DIMENSION,\n",
        "            EMBEDDING_DIMENSION,\n",
        "            HIDDEN_DIMENSION,\n",
        "            OUTPUT_DIMENSION,\n",
        "            DROPOUT,\n",
        "            PAD_INDEX)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yUd0tN3Cznid",
        "outputId": "a3706e51-2f00-47ca-d349-9da32415a62d"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:71: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
            "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "UNK_INDEX = TEXT_FIELD.vocab.stoi[TEXT_FIELD.unk_token]\n",
        "\n",
        "lstm_model.embedding_layer.weight.data[UNK_INDEX] = torch.zeros(EMBEDDING_DIMENSION)\n",
        "lstm_model.embedding_layer.weight.data[PAD_INDEX] = torch.zeros(EMBEDDING_DIMENSION)"
      ],
      "metadata": {
        "id": "0l6fRMbRzqo9"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optim = torch.optim.Adam(lstm_model.parameters())\n",
        "loss_func = nn.BCEWithLogitsLoss()\n",
        "\n",
        "lstm_model = lstm_model.to(device)\n",
        "loss_func = loss_func.to(device)"
      ],
      "metadata": {
        "id": "ANkqqb32zurs"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def accuracy_metric(predictions, ground_truth):\n",
        "    \"\"\"\n",
        "    Returns 0-1 accuracy for the given set of predictions and ground truth\n",
        "    \"\"\"\n",
        "    # round predictions to either 0 or 1\n",
        "    rounded_predictions = torch.round(torch.sigmoid(predictions))\n",
        "    success = (rounded_predictions == ground_truth).float() #convert into float for division\n",
        "    accuracy = success.sum() / len(success)\n",
        "    return accuracy"
      ],
      "metadata": {
        "id": "gw-FN7WNzvVt"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, data_iterator, optim, loss_func):\n",
        "    loss = 0\n",
        "    accuracy = 0\n",
        "    model.train()\n",
        "\n",
        "    for curr_batch in data_iterator:\n",
        "        optim.zero_grad()\n",
        "        sequence, sequence_lengths = curr_batch.text\n",
        "        preds = lstm_model(sequence, sequence_lengths).squeeze(1)\n",
        "\n",
        "        loss_curr = loss_func(preds, curr_batch.label)\n",
        "        accuracy_curr = accuracy_metric(preds, curr_batch.label)\n",
        "\n",
        "        loss_curr.backward()\n",
        "        optim.step()\n",
        "\n",
        "        loss += loss_curr.item()\n",
        "        accuracy += accuracy_curr.item()\n",
        "\n",
        "    return loss/len(data_iterator), accuracy/len(data_iterator)"
      ],
      "metadata": {
        "id": "LkYPubB0zw8V"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def validate(model, data_iterator, loss_func):\n",
        "    loss = 0\n",
        "    accuracy = 0\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for curr_batch in data_iterator:\n",
        "            sequence, sequence_lengths = curr_batch.text\n",
        "            preds = model(sequence, sequence_lengths).squeeze(1)\n",
        "\n",
        "            loss_curr = loss_func(preds, curr_batch.label)\n",
        "            accuracy_curr = accuracy_metric(preds, curr_batch.label)\n",
        "\n",
        "            loss += loss_curr.item()\n",
        "            accuracy += accuracy_curr.item()\n",
        "\n",
        "    return loss/len(data_iterator), accuracy/len(data_iterator)"
      ],
      "metadata": {
        "id": "1Kb3tgz8zyVH"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 10\n",
        "best_validation_loss = float('inf')\n",
        "\n",
        "for ep in range(num_epochs):\n",
        "\n",
        "    time_start = time.time()\n",
        "\n",
        "    training_loss, train_accuracy = train(lstm_model, train_data_iterator, optim, loss_func)\n",
        "    validation_loss, validation_accuracy = validate(lstm_model, valid_data_iterator, loss_func)\n",
        "\n",
        "    time_end = time.time()\n",
        "    time_delta = time_end - time_start\n",
        "\n",
        "    if validation_loss < best_validation_loss:\n",
        "        best_validation_loss = validation_loss\n",
        "        torch.save(lstm_model.state_dict(), 'lstm_model.pt')\n",
        "\n",
        "    print(f'epoch number: {ep+1} | time elapsed: {time_delta}s')\n",
        "    print(f'training loss: {training_loss:.3f} | training accuracy: {train_accuracy*100:.2f}%')\n",
        "    print(f'validation loss: {validation_loss:.3f} |  validation accuracy: {validation_accuracy*100:.2f}%')\n",
        "    print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kaVyxR8cz0pd",
        "outputId": "d4455f98-ba3e-41e9-9e76-a1e76ea195a9"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch number: 1 | time elapsed: 10.536290407180786s\n",
            "training loss: 0.688 | training accuracy: 53.98%\n",
            "validation loss: 0.671 |  validation accuracy: 59.79%\n",
            "\n",
            "epoch number: 2 | time elapsed: 10.250645637512207s\n",
            "training loss: 0.657 | training accuracy: 60.31%\n",
            "validation loss: 0.588 |  validation accuracy: 69.64%\n",
            "\n",
            "epoch number: 3 | time elapsed: 8.079545974731445s\n",
            "training loss: 0.584 | training accuracy: 69.34%\n",
            "validation loss: 0.732 |  validation accuracy: 68.91%\n",
            "\n",
            "epoch number: 4 | time elapsed: 9.13801908493042s\n",
            "training loss: 0.535 | training accuracy: 73.45%\n",
            "validation loss: 0.530 |  validation accuracy: 72.25%\n",
            "\n",
            "epoch number: 5 | time elapsed: 9.264939785003662s\n",
            "training loss: 0.484 | training accuracy: 76.79%\n",
            "validation loss: 0.537 |  validation accuracy: 72.28%\n",
            "\n",
            "epoch number: 6 | time elapsed: 10.407737255096436s\n",
            "training loss: 0.447 | training accuracy: 79.34%\n",
            "validation loss: 0.576 |  validation accuracy: 74.81%\n",
            "\n",
            "epoch number: 7 | time elapsed: 9.219055414199829s\n",
            "training loss: 0.430 | training accuracy: 80.13%\n",
            "validation loss: 0.525 |  validation accuracy: 75.52%\n",
            "\n",
            "epoch number: 8 | time elapsed: 9.725209712982178s\n",
            "training loss: 0.441 | training accuracy: 80.42%\n",
            "validation loss: 0.627 |  validation accuracy: 74.78%\n",
            "\n",
            "epoch number: 9 | time elapsed: 9.837013006210327s\n",
            "training loss: 0.410 | training accuracy: 82.19%\n",
            "validation loss: 0.606 |  validation accuracy: 76.49%\n",
            "\n",
            "epoch number: 10 | time elapsed: 10.034965991973877s\n",
            "training loss: 0.429 | training accuracy: 80.97%\n",
            "validation loss: 0.547 |  validation accuracy: 79.45%\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#lstm_model.load_state_dict(torch.load('../../mastering_pytorch_packt/04_deep_recurrent_net_architectures/lstm_model.pt'))\n",
        "lstm_model.load_state_dict(torch.load('lstm_model.pt'))\n",
        "\n",
        "test_loss, test_accuracy = validate(lstm_model, test_data_iterator, loss_func)\n",
        "\n",
        "print(f'test loss: {test_loss:.3f} | test accuracy: {test_accuracy*100:.2f}%')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PRu_7Iifz2xc",
        "outputId": "5f2b0e8e-3bca-4be7-c24f-6a95813a375e"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "test loss: 0.549 | test accuracy: 74.22%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def sentiment_inference(model, sentence):\n",
        "    model.eval()\n",
        "\n",
        "    # text transformations\n",
        "    tokenized = data.get_tokenizer(\"basic_english\")(sentence)\n",
        "    tokenized = [TEXT_FIELD.vocab.stoi[t] for t in tokenized]\n",
        "\n",
        "    # model inference\n",
        "    model_input = torch.LongTensor(tokenized).to(device)\n",
        "    model_input = model_input.unsqueeze(1)\n",
        "\n",
        "    pred = torch.sigmoid(model(model_input))\n",
        "\n",
        "    return pred.item()"
      ],
      "metadata": {
        "id": "j-hMX8_Kz4bU"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(sentiment_inference(lstm_model, \"This film is horrible\"))\n",
        "print(sentiment_inference(lstm_model, \"Director tried too hard but this film is bad\"))\n",
        "print(sentiment_inference(lstm_model, \"Decent movie, although could be shorter\"))\n",
        "print(sentiment_inference(lstm_model, \"This film will be houseful for weeks\"))\n",
        "print(sentiment_inference(lstm_model, \"I loved the movie, every part of it\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TlkQl9Bnz5dm",
        "outputId": "2310b7de-97e6-43ac-cee4-8fb21d5c3bf3"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.06481477618217468\n",
            "0.06424909085035324\n",
            "0.3703685998916626\n",
            "0.5785183310508728\n",
            "0.9535890817642212\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## GRU와 Attention 기반 모델"
      ],
      "metadata": {
        "id": "jmNH2tbR0CbE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### GRU와 PyTorch"
      ],
      "metadata": {
        "id": "JwTFhV8e0GHl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "GRU는 두 개의 게이트(리셋 게이트와 업데이트 게이트)와 하나의 은닉 상태 벡터로 구성된 일종의 메모리 셀이다. 구성 측면에서 GRU는 LSTM보다 단순하지만 경사가 폭발하거나 소실하는 문제를 처리하는 데 있어 똑같이 효과적이다.  \n",
        "GRU는 LSTM보다 훈련 속도가 빠르고 언어 모델링 같은 수많은 작업에서 훨씬 적은 훈련 데이터로 LSTM만큼 수행할 수 있다.  \n",
        "파이토치는 코드 한 줄로 GRU 계층을 인스턴스화하는 nn.GRU 모듈을 제공한다.  \n",
        "\n",
        "\n",
        "```\n",
        "self.gru_layer = nn.GRU(\n",
        "  input_size, hidden_size, num_layer=2, dropout=0.8, bidirectional=True\n",
        ")\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "CneqUIvKz7IE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Attention 기반 모델"
      ],
      "metadata": {
        "id": "O_wBRN7D0aNP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![Attention RNN architecture](https://miro.medium.com/v2/resize:fit:1200/1*TPlS-uko-n3uAxbAQY_STQ.png)"
      ],
      "metadata": {
        "id": "HbmUmJdK2NA2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Attention 개념은 우리 인간이 때에 따라, 또 sequence(text)의 어느 부분인지에 따라 주의(attention)을 기울이는 정도가 다르다는 점에 착안했다.  \n",
        "예를 들어 'Martha sings beautifully, I am hooked to ___ voice.'라는 문장을 완성한다면, 채워야 할 단어가 'her'라는 것을 추측하기 위해 'Martha'라는 단어에 더 주의를 기울인다. 반면, 우리가 완성해야 할 문장이 'Martha sings beautifully, I am hooked to her ___.'라면 채워야 할 단어로 'voice', 'songs', 'sining' 등을 추측하기 위해 단어 'sings'에 더 주의를 기울일 것이다."
      ],
      "metadata": {
        "id": "lgfx9bOG0aLH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "모든 recurrent network 아키텍처에는 현 시간 단계에서 출력을 예측하기 위해 sequence의 특정 부분에 초점을 맞추는 메커니즘은 존재하지 않는다. 대신 RNN은 hidden state vector 형태로 과거 sequence의 요약만 얻을 수 있다."
      ],
      "metadata": {
        "id": "-1yor3vJ0ZM_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "이 아키텍처에서 전역 컨텍스트 벡터는 매시간 단계마다 계산된다. 이후 앞서 나온 모든 단어에 주의를 기울이는 것이 아니라 앞서 나온 k개 단어에만 주의를 기울이는 로컬 컨텍스트 벡터를 사용하는 형태로 아키텍처의 변형이 개발됐다."
      ],
      "metadata": {
        "id": "CsBzYqzY0Vj4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "순환망은 시간에 따라 펼쳐야 해서 병렬 처리가 불가능했다. 그렇지만 transformer 모델이라는 새로운 모델은 순환 계층과 합성곱 계층이 없어 병렬 처리가 가능하고 계산량 측면에서 가볍다."
      ],
      "metadata": {
        "id": "kQIMPtRs17Nx"
      }
    }
  ]
}