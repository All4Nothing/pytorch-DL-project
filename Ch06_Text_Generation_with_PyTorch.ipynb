{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNSuKvjio5VtxykSoEZGgKS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/All4Nothing/pytorch-DL-project/blob/main/Ch06_Text_Generation_with_PyTorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Ch06 - (1)"
      ],
      "metadata": {
        "id": "ySvimmpozjpt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Text Generation with Transformer"
      ],
      "metadata": {
        "id": "PHVVopam0qV6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchtext==0.6.0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 670
        },
        "id": "hynQGtPW1Xjv",
        "outputId": "774e8d61-127c-44ae-82f9-185d2eb7dd6a"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torchtext==0.6.0\n",
            "  Downloading torchtext-0.6.0-py3-none-any.whl (64 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/64.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.2/64.2 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torchtext==0.6.0) (4.66.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchtext==0.6.0) (2.31.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from torchtext==0.6.0) (2.1.0+cu118)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchtext==0.6.0) (1.23.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from torchtext==0.6.0) (1.16.0)\n",
            "Collecting sentencepiece (from torchtext==0.6.0)\n",
            "  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext==0.6.0) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext==0.6.0) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext==0.6.0) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext==0.6.0) (2023.7.22)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->torchtext==0.6.0) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->torchtext==0.6.0) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->torchtext==0.6.0) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->torchtext==0.6.0) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->torchtext==0.6.0) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->torchtext==0.6.0) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch->torchtext==0.6.0) (2.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->torchtext==0.6.0) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->torchtext==0.6.0) (1.3.0)\n",
            "Installing collected packages: sentencepiece, torchtext\n",
            "  Attempting uninstall: torchtext\n",
            "    Found existing installation: torchtext 0.16.0\n",
            "    Uninstalling torchtext-0.16.0:\n",
            "      Successfully uninstalled torchtext-0.16.0\n",
            "Successfully installed sentencepiece-0.1.99 torchtext-0.6.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "torchtext"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "MkcBJ1x4zh04"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import time\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
        "\n",
        "import torchtext\n",
        "from torchtext.data.utils import get_tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Transformer(nn.Module):\n",
        "    def __init__(self, num_token, num_inputs, num_heads, num_hidden, num_layers, dropout=0.3):\n",
        "        super(Transformer, self).__init__()\n",
        "        self.model_name = 'transformer'\n",
        "        self.mask_source = None\n",
        "        self.position_enc = PosEnc(num_inputs, dropout)\n",
        "        layers_enc = TransformerEncoderLayer(num_inputs, num_heads, num_hidden, dropout)\n",
        "        self.enc_transformer = TransformerEncoder(layers_enc, num_layers)\n",
        "        self.enc = nn.Embedding(num_token, num_inputs)\n",
        "        self.num_inputs = num_inputs\n",
        "        self.dec = nn.Linear(num_inputs, num_token)\n",
        "        self.init_params()\n",
        "\n",
        "    def _gen_sqr_nxt_mask(self, size):\n",
        "        msk = (torch.triu(torch.ones(size, size)) == 1).transpose(0, 1)\n",
        "        msk = msk.float().masked_fill(msk == 0, float('-inf'))\n",
        "        msk = msk.masked_fill(msk == 1, float(0.0))\n",
        "        return msk\n",
        "\n",
        "    def init_params(self):\n",
        "        initial_rng = 0.12\n",
        "        self.enc.weight.data.uniform_(-initial_rng, initial_rng)\n",
        "        self.dec.bias.data.zero_()\n",
        "        self.dec.weight.data.uniform_(-initial_rng, initial_rng)\n",
        "\n",
        "    def forward(self, source):\n",
        "        if self.mask_source is None or self.mask_source.size(0) != len(source):\n",
        "            dvc = source.device\n",
        "            msk = self._gen_sqr_nxt_mask(len(source)).to(dvc)\n",
        "            self.mask_source = msk\n",
        "\n",
        "        source = self.enc(source) * math.sqrt(self.num_inputs)\n",
        "        source = self.position_enc(source)\n",
        "        op = self.enc_transformer(source, self.mask_source)\n",
        "        op = self.dec(op)\n",
        "        return op"
      ],
      "metadata": {
        "id": "zKlKPqjT0tdi"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PosEnc(nn.Module):\n",
        "    def __init__(self, d_m, dropout=0.2, size_limit=5000):\n",
        "        super(PosEnc, self).__init__()\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        p_enc = torch.zeros(size_limit, d_m)\n",
        "        pos = torch.arange(0, size_limit, dtype=torch.float).unsqueeze(1)\n",
        "        divider = torch.exp(torch.arange(0, d_m, 2).float() * (-math.log(10000.0) / d_m))\n",
        "        p_enc[:, 0::2] = torch.sin(pos * divider)\n",
        "        p_enc[:, 1::2] = torch.cos(pos * divider)\n",
        "        p_enc = p_enc.unsqueeze(0).transpose(0, 1)\n",
        "        self.register_buffer('p_enc', p_enc)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.dropout(x + self.p_enc[:x.size(0), :])"
      ],
      "metadata": {
        "id": "SoLVJtlg1G_3"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "TEXT = torchtext.data.Field(tokenize=get_tokenizer(\"basic_english\"), lower=True, eos_token='<eos>', init_token='<sos>')\n",
        "training_text, validation_text, testing_text = torchtext.datasets.WikiText2.splits(TEXT)\n",
        "TEXT.build_vocab(training_text)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "def gen_batches(text_dataset, batch_size):\n",
        "    text_dataset = TEXT.numericalize([text_dataset.examples[0].text])\n",
        "    # divide text dataset into parts of size equal to batch_size\n",
        "    num_batches = text_dataset.size(0) // batch_size\n",
        "    # remove data points that lie outside batches (remainders)\n",
        "    text_dataset = text_dataset.narrow(0, 0, num_batches * batch_size)\n",
        "    # distribute dataset across batches evenly\n",
        "    text_dataset = text_dataset.view(batch_size, -1).t().contiguous()\n",
        "    return text_dataset.to(device)\n",
        "\n",
        "training_batch_size = 32\n",
        "evaluation_batch_size = 16\n",
        "\n",
        "training_data = gen_batches(training_text, training_batch_size)\n",
        "validation_data = gen_batches(validation_text, evaluation_batch_size)\n",
        "testing_data = gen_batches(testing_text, evaluation_batch_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rMdRzBXh1HRA",
        "outputId": "0a646883-f7a6-4c44-946a-cbd53a0930a0"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "downloading wikitext-2-v1.zip\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "wikitext-2-v1.zip: 100%|██████████| 4.48M/4.48M [00:00<00:00, 9.00MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "extracting\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "max_seq_len = 64\n",
        "def return_batch(src, k):\n",
        "    sequence_length = min(max_seq_len, len(src) - 1 - k)\n",
        "    sequence_data = src[k:k+sequence_length]\n",
        "    sequence_label = src[k+1:k+1+sequence_length].view(-1)\n",
        "    return sequence_data, sequence_label"
      ],
      "metadata": {
        "id": "p7pgfyPp1NDu"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_tokens = len(TEXT.vocab.stoi) # vocabulary size\n",
        "embedding_size = 256 # dimension of embedding layer\n",
        "num_hidden_params = 256 # transformer encoder's hidden (feed forward) layer dimension\n",
        "num_layers = 2 # num of transformer encoder layers within transformer encoder\n",
        "num_heads = 2 # num of heads in (multi head) attention models\n",
        "dropout = 0.25 # value (fraction) of dropout\n",
        "loss_func = nn.CrossEntropyLoss()\n",
        "lrate = 4.0 # learning rate\n",
        "transformer_model = Transformer(num_tokens, embedding_size, num_heads, num_hidden_params, num_layers,\n",
        "                                     dropout).to(device)\n",
        "optim_module = torch.optim.SGD(transformer_model.parameters(), lr=lrate)\n",
        "sched_module = torch.optim.lr_scheduler.StepLR(optim_module, 1.0, gamma=0.88)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lSToU9br1no3",
        "outputId": "8cf0f434-27a1-421c-e910-f8290887b861"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/transformer.py:282: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
            "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model():\n",
        "    transformer_model.train()\n",
        "    loss_total = 0.\n",
        "    time_start = time.time()\n",
        "    num_tokens = len(TEXT.vocab.stoi)\n",
        "    for b, i in enumerate(range(0, training_data.size(0) - 1, max_seq_len)):\n",
        "        train_data_batch, train_label_batch = return_batch(training_data, i)\n",
        "        optim_module.zero_grad()\n",
        "        op = transformer_model(train_data_batch)\n",
        "        loss_curr = loss_func(op.view(-1, num_tokens), train_label_batch)\n",
        "        loss_curr.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(transformer_model.parameters(), 0.6)\n",
        "        optim_module.step()\n",
        "\n",
        "        loss_total += loss_curr.item()\n",
        "        interval = 100\n",
        "        if b % interval == 0 and b > 0:\n",
        "            loss_interval = loss_total / interval\n",
        "            time_delta = time.time() - time_start\n",
        "            print(f\"epoch {ep}, {b}/{len(training_data)//max_seq_len} batches, training loss {loss_interval:.2f}, training perplexity {math.exp(loss_interval):.2f}\")\n",
        "            loss_total = 0\n",
        "            time_start = time.time()\n",
        "\n",
        "def eval_model(eval_model_obj, eval_data_source):\n",
        "    eval_model_obj.eval()\n",
        "    loss_total = 0.\n",
        "    num_tokens = len(TEXT.vocab.stoi)\n",
        "    with torch.no_grad():\n",
        "        for j in range(0, eval_data_source.size(0) - 1, max_seq_len):\n",
        "            eval_data, eval_label = return_batch(eval_data_source, j)\n",
        "            op = eval_model_obj(eval_data)\n",
        "            op_flat = op.view(-1, num_tokens)\n",
        "            loss_total += len(eval_data) * loss_func(op_flat, eval_label).item()\n",
        "    return loss_total / (len(eval_data_source) - 1)"
      ],
      "metadata": {
        "id": "5k2kEaqv1tV3"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "min_validation_loss = float(\"inf\")\n",
        "eps = 50\n",
        "best_model_so_far = None\n",
        "\n",
        "for ep in range(1, eps + 1):\n",
        "    ep_time_start = time.time()\n",
        "    train_model()\n",
        "    validation_loss = eval_model(transformer_model, validation_data)\n",
        "    print()\n",
        "    print(f\"epoch {ep:}, validation loss {validation_loss:.2f}, validation perplexity {math.exp(validation_loss):.2f}\")\n",
        "    print()\n",
        "\n",
        "    if validation_loss < min_validation_loss:\n",
        "        min_validation_loss = validation_loss\n",
        "        best_model_so_far = transformer_model\n",
        "\n",
        "    sched_module.step()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "guj4xQXg10OQ",
        "outputId": "a5d63bc7-a828-4e5b-c0d5-a37e29aaa00e"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 1, 100/1018 batches, training loss 8.78, training perplexity 6471.43\n",
            "epoch 1, 200/1018 batches, training loss 7.24, training perplexity 1389.52\n",
            "epoch 1, 300/1018 batches, training loss 6.84, training perplexity 937.78\n",
            "epoch 1, 400/1018 batches, training loss 6.58, training perplexity 723.86\n",
            "epoch 1, 500/1018 batches, training loss 6.49, training perplexity 657.64\n",
            "epoch 1, 600/1018 batches, training loss 6.35, training perplexity 569.80\n",
            "epoch 1, 700/1018 batches, training loss 6.25, training perplexity 516.92\n",
            "epoch 1, 800/1018 batches, training loss 6.14, training perplexity 465.15\n",
            "epoch 1, 900/1018 batches, training loss 6.10, training perplexity 445.41\n",
            "epoch 1, 1000/1018 batches, training loss 6.07, training perplexity 433.50\n",
            "\n",
            "epoch 1, validation loss 5.77, validation perplexity 320.03\n",
            "\n",
            "epoch 2, 100/1018 batches, training loss 6.00, training perplexity 402.62\n",
            "epoch 2, 200/1018 batches, training loss 5.91, training perplexity 366.95\n",
            "epoch 2, 300/1018 batches, training loss 5.84, training perplexity 345.49\n",
            "epoch 2, 400/1018 batches, training loss 5.80, training perplexity 330.47\n",
            "epoch 2, 500/1018 batches, training loss 5.83, training perplexity 339.78\n",
            "epoch 2, 600/1018 batches, training loss 5.78, training perplexity 323.45\n",
            "epoch 2, 700/1018 batches, training loss 5.78, training perplexity 323.24\n",
            "epoch 2, 800/1018 batches, training loss 5.66, training perplexity 287.11\n",
            "epoch 2, 900/1018 batches, training loss 5.69, training perplexity 295.15\n",
            "epoch 2, 1000/1018 batches, training loss 5.71, training perplexity 302.73\n",
            "\n",
            "epoch 2, validation loss 5.53, validation perplexity 251.69\n",
            "\n",
            "epoch 3, 100/1018 batches, training loss 5.68, training perplexity 291.80\n",
            "epoch 3, 200/1018 batches, training loss 5.61, training perplexity 273.12\n",
            "epoch 3, 300/1018 batches, training loss 5.56, training perplexity 260.36\n",
            "epoch 3, 400/1018 batches, training loss 5.53, training perplexity 252.11\n",
            "epoch 3, 500/1018 batches, training loss 5.56, training perplexity 259.43\n",
            "epoch 3, 600/1018 batches, training loss 5.53, training perplexity 251.68\n",
            "epoch 3, 700/1018 batches, training loss 5.54, training perplexity 254.71\n",
            "epoch 3, 800/1018 batches, training loss 5.40, training perplexity 221.36\n",
            "epoch 3, 900/1018 batches, training loss 5.45, training perplexity 232.38\n",
            "epoch 3, 1000/1018 batches, training loss 5.49, training perplexity 242.58\n",
            "\n",
            "epoch 3, validation loss 5.41, validation perplexity 223.75\n",
            "\n",
            "epoch 4, 100/1018 batches, training loss 5.47, training perplexity 237.62\n",
            "epoch 4, 200/1018 batches, training loss 5.41, training perplexity 223.36\n",
            "epoch 4, 300/1018 batches, training loss 5.38, training perplexity 216.60\n",
            "epoch 4, 400/1018 batches, training loss 5.35, training perplexity 209.76\n",
            "epoch 4, 500/1018 batches, training loss 5.37, training perplexity 215.61\n",
            "epoch 4, 600/1018 batches, training loss 5.35, training perplexity 211.55\n",
            "epoch 4, 700/1018 batches, training loss 5.37, training perplexity 215.06\n",
            "epoch 4, 800/1018 batches, training loss 5.22, training perplexity 184.15\n",
            "epoch 4, 900/1018 batches, training loss 5.27, training perplexity 194.84\n",
            "epoch 4, 1000/1018 batches, training loss 5.33, training perplexity 205.91\n",
            "\n",
            "epoch 4, validation loss 5.30, validation perplexity 200.76\n",
            "\n",
            "epoch 5, 100/1018 batches, training loss 5.32, training perplexity 203.80\n",
            "epoch 5, 200/1018 batches, training loss 5.25, training perplexity 190.71\n",
            "epoch 5, 300/1018 batches, training loss 5.23, training perplexity 187.05\n",
            "epoch 5, 400/1018 batches, training loss 5.21, training perplexity 183.43\n",
            "epoch 5, 500/1018 batches, training loss 5.23, training perplexity 187.09\n",
            "epoch 5, 600/1018 batches, training loss 5.21, training perplexity 183.86\n",
            "epoch 5, 700/1018 batches, training loss 5.24, training perplexity 187.88\n",
            "epoch 5, 800/1018 batches, training loss 5.07, training perplexity 159.20\n",
            "epoch 5, 900/1018 batches, training loss 5.14, training perplexity 171.10\n",
            "epoch 5, 1000/1018 batches, training loss 5.19, training perplexity 179.38\n",
            "\n",
            "epoch 5, validation loss 5.23, validation perplexity 186.99\n",
            "\n",
            "epoch 6, 100/1018 batches, training loss 5.19, training perplexity 179.80\n",
            "epoch 6, 200/1018 batches, training loss 5.13, training perplexity 168.55\n",
            "epoch 6, 300/1018 batches, training loss 5.11, training perplexity 165.37\n",
            "epoch 6, 400/1018 batches, training loss 5.09, training perplexity 162.74\n",
            "epoch 6, 500/1018 batches, training loss 5.11, training perplexity 165.49\n",
            "epoch 6, 600/1018 batches, training loss 5.10, training perplexity 163.48\n",
            "epoch 6, 700/1018 batches, training loss 5.12, training perplexity 166.87\n",
            "epoch 6, 800/1018 batches, training loss 4.95, training perplexity 141.52\n",
            "epoch 6, 900/1018 batches, training loss 5.03, training perplexity 152.38\n",
            "epoch 6, 1000/1018 batches, training loss 5.07, training perplexity 158.97\n",
            "\n",
            "epoch 6, validation loss 5.19, validation perplexity 178.96\n",
            "\n",
            "epoch 7, 100/1018 batches, training loss 5.09, training perplexity 161.65\n",
            "epoch 7, 200/1018 batches, training loss 5.02, training perplexity 151.11\n",
            "epoch 7, 300/1018 batches, training loss 5.01, training perplexity 149.25\n",
            "epoch 7, 400/1018 batches, training loss 4.99, training perplexity 147.50\n",
            "epoch 7, 500/1018 batches, training loss 5.01, training perplexity 149.84\n",
            "epoch 7, 600/1018 batches, training loss 5.00, training perplexity 147.91\n",
            "epoch 7, 700/1018 batches, training loss 5.02, training perplexity 151.29\n",
            "epoch 7, 800/1018 batches, training loss 4.85, training perplexity 128.10\n",
            "epoch 7, 900/1018 batches, training loss 4.92, training perplexity 137.47\n",
            "epoch 7, 1000/1018 batches, training loss 4.97, training perplexity 143.82\n",
            "\n",
            "epoch 7, validation loss 5.16, validation perplexity 173.37\n",
            "\n",
            "epoch 8, 100/1018 batches, training loss 4.99, training perplexity 146.66\n",
            "epoch 8, 200/1018 batches, training loss 4.92, training perplexity 137.52\n",
            "epoch 8, 300/1018 batches, training loss 4.91, training perplexity 136.28\n",
            "epoch 8, 400/1018 batches, training loss 4.90, training perplexity 134.65\n",
            "epoch 8, 500/1018 batches, training loss 4.92, training perplexity 136.73\n",
            "epoch 8, 600/1018 batches, training loss 4.91, training perplexity 135.42\n",
            "epoch 8, 700/1018 batches, training loss 4.93, training perplexity 138.37\n",
            "epoch 8, 800/1018 batches, training loss 4.76, training perplexity 117.23\n",
            "epoch 8, 900/1018 batches, training loss 4.84, training perplexity 126.05\n",
            "epoch 8, 1000/1018 batches, training loss 4.88, training perplexity 132.24\n",
            "\n",
            "epoch 8, validation loss 5.13, validation perplexity 168.25\n",
            "\n",
            "epoch 9, 100/1018 batches, training loss 4.91, training perplexity 135.86\n",
            "epoch 9, 200/1018 batches, training loss 4.84, training perplexity 126.08\n",
            "epoch 9, 300/1018 batches, training loss 4.83, training perplexity 125.61\n",
            "epoch 9, 400/1018 batches, training loss 4.83, training perplexity 125.31\n",
            "epoch 9, 500/1018 batches, training loss 4.84, training perplexity 126.63\n",
            "epoch 9, 600/1018 batches, training loss 4.84, training perplexity 125.88\n",
            "epoch 9, 700/1018 batches, training loss 4.86, training perplexity 128.47\n",
            "epoch 9, 800/1018 batches, training loss 4.69, training perplexity 108.71\n",
            "epoch 9, 900/1018 batches, training loss 4.76, training perplexity 117.08\n",
            "epoch 9, 1000/1018 batches, training loss 4.80, training perplexity 122.07\n",
            "\n",
            "epoch 9, validation loss 5.09, validation perplexity 163.02\n",
            "\n",
            "epoch 10, 100/1018 batches, training loss 4.84, training perplexity 126.88\n",
            "epoch 10, 200/1018 batches, training loss 4.77, training perplexity 117.79\n",
            "epoch 10, 300/1018 batches, training loss 4.77, training perplexity 117.78\n",
            "epoch 10, 400/1018 batches, training loss 4.76, training perplexity 117.21\n",
            "epoch 10, 500/1018 batches, training loss 4.78, training perplexity 118.60\n",
            "epoch 10, 600/1018 batches, training loss 4.77, training perplexity 117.92\n",
            "epoch 10, 700/1018 batches, training loss 4.79, training perplexity 120.45\n",
            "epoch 10, 800/1018 batches, training loss 4.63, training perplexity 102.18\n",
            "epoch 10, 900/1018 batches, training loss 4.70, training perplexity 110.21\n",
            "epoch 10, 1000/1018 batches, training loss 4.74, training perplexity 114.58\n",
            "\n",
            "epoch 10, validation loss 5.08, validation perplexity 161.18\n",
            "\n",
            "epoch 11, 100/1018 batches, training loss 4.78, training perplexity 119.28\n",
            "epoch 11, 200/1018 batches, training loss 4.71, training perplexity 110.96\n",
            "epoch 11, 300/1018 batches, training loss 4.71, training perplexity 111.33\n",
            "epoch 11, 400/1018 batches, training loss 4.71, training perplexity 110.96\n",
            "epoch 11, 500/1018 batches, training loss 4.72, training perplexity 112.00\n",
            "epoch 11, 600/1018 batches, training loss 4.72, training perplexity 111.72\n",
            "epoch 11, 700/1018 batches, training loss 4.74, training perplexity 114.13\n",
            "epoch 11, 800/1018 batches, training loss 4.57, training perplexity 96.58\n",
            "epoch 11, 900/1018 batches, training loss 4.65, training perplexity 104.16\n",
            "epoch 11, 1000/1018 batches, training loss 4.68, training perplexity 108.23\n",
            "\n",
            "epoch 11, validation loss 5.07, validation perplexity 159.81\n",
            "\n",
            "epoch 12, 100/1018 batches, training loss 4.73, training perplexity 113.53\n",
            "epoch 12, 200/1018 batches, training loss 4.66, training perplexity 105.73\n",
            "epoch 12, 300/1018 batches, training loss 4.67, training perplexity 106.23\n",
            "epoch 12, 400/1018 batches, training loss 4.66, training perplexity 105.56\n",
            "epoch 12, 500/1018 batches, training loss 4.67, training perplexity 106.71\n",
            "epoch 12, 600/1018 batches, training loss 4.67, training perplexity 106.21\n",
            "epoch 12, 700/1018 batches, training loss 4.69, training perplexity 108.57\n",
            "epoch 12, 800/1018 batches, training loss 4.52, training perplexity 92.24\n",
            "epoch 12, 900/1018 batches, training loss 4.60, training perplexity 99.53\n",
            "epoch 12, 1000/1018 batches, training loss 4.64, training perplexity 103.04\n",
            "\n",
            "epoch 12, validation loss 5.06, validation perplexity 157.33\n",
            "\n",
            "epoch 13, 100/1018 batches, training loss 4.69, training perplexity 108.96\n",
            "epoch 13, 200/1018 batches, training loss 4.62, training perplexity 101.29\n",
            "epoch 13, 300/1018 batches, training loss 4.62, training perplexity 101.82\n",
            "epoch 13, 400/1018 batches, training loss 4.62, training perplexity 101.45\n",
            "epoch 13, 500/1018 batches, training loss 4.63, training perplexity 102.31\n",
            "epoch 13, 600/1018 batches, training loss 4.63, training perplexity 102.18\n",
            "epoch 13, 700/1018 batches, training loss 4.65, training perplexity 104.46\n",
            "epoch 13, 800/1018 batches, training loss 4.48, training perplexity 88.56\n",
            "epoch 13, 900/1018 batches, training loss 4.56, training perplexity 95.75\n",
            "epoch 13, 1000/1018 batches, training loss 4.59, training perplexity 98.79\n",
            "\n",
            "epoch 13, validation loss 5.05, validation perplexity 156.09\n",
            "\n",
            "epoch 14, 100/1018 batches, training loss 4.65, training perplexity 104.88\n",
            "epoch 14, 200/1018 batches, training loss 4.58, training perplexity 97.70\n",
            "epoch 14, 300/1018 batches, training loss 4.59, training perplexity 98.14\n",
            "epoch 14, 400/1018 batches, training loss 4.58, training perplexity 97.95\n",
            "epoch 14, 500/1018 batches, training loss 4.59, training perplexity 98.95\n",
            "epoch 14, 600/1018 batches, training loss 4.59, training perplexity 98.77\n",
            "epoch 14, 700/1018 batches, training loss 4.61, training perplexity 100.60\n",
            "epoch 14, 800/1018 batches, training loss 4.45, training perplexity 86.02\n",
            "epoch 14, 900/1018 batches, training loss 4.52, training perplexity 92.20\n",
            "epoch 14, 1000/1018 batches, training loss 4.56, training perplexity 95.31\n",
            "\n",
            "epoch 14, validation loss 5.04, validation perplexity 154.99\n",
            "\n",
            "epoch 15, 100/1018 batches, training loss 4.62, training perplexity 101.31\n",
            "epoch 15, 200/1018 batches, training loss 4.54, training perplexity 93.92\n",
            "epoch 15, 300/1018 batches, training loss 4.56, training perplexity 95.30\n",
            "epoch 15, 400/1018 batches, training loss 4.55, training perplexity 95.08\n",
            "epoch 15, 500/1018 batches, training loss 4.56, training perplexity 95.85\n",
            "epoch 15, 600/1018 batches, training loss 4.56, training perplexity 95.59\n",
            "epoch 15, 700/1018 batches, training loss 4.58, training perplexity 97.40\n",
            "epoch 15, 800/1018 batches, training loss 4.42, training perplexity 83.50\n",
            "epoch 15, 900/1018 batches, training loss 4.49, training perplexity 89.55\n",
            "epoch 15, 1000/1018 batches, training loss 4.53, training perplexity 92.51\n",
            "\n",
            "epoch 15, validation loss 5.04, validation perplexity 154.02\n",
            "\n",
            "epoch 16, 100/1018 batches, training loss 4.59, training perplexity 98.46\n",
            "epoch 16, 200/1018 batches, training loss 4.52, training perplexity 91.54\n",
            "epoch 16, 300/1018 batches, training loss 4.54, training perplexity 93.26\n",
            "epoch 16, 400/1018 batches, training loss 4.53, training perplexity 92.73\n",
            "epoch 16, 500/1018 batches, training loss 4.53, training perplexity 93.10\n",
            "epoch 16, 600/1018 batches, training loss 4.53, training perplexity 92.98\n",
            "epoch 16, 700/1018 batches, training loss 4.55, training perplexity 94.63\n",
            "epoch 16, 800/1018 batches, training loss 4.40, training perplexity 81.26\n",
            "epoch 16, 900/1018 batches, training loss 4.47, training perplexity 87.44\n",
            "epoch 16, 1000/1018 batches, training loss 4.50, training perplexity 89.82\n",
            "\n",
            "epoch 16, validation loss 5.05, validation perplexity 155.44\n",
            "\n",
            "epoch 17, 100/1018 batches, training loss 4.57, training perplexity 96.42\n",
            "epoch 17, 200/1018 batches, training loss 4.49, training perplexity 89.44\n",
            "epoch 17, 300/1018 batches, training loss 4.51, training perplexity 90.68\n",
            "epoch 17, 400/1018 batches, training loss 4.51, training perplexity 90.60\n",
            "epoch 17, 500/1018 batches, training loss 4.51, training perplexity 91.10\n",
            "epoch 17, 600/1018 batches, training loss 4.51, training perplexity 90.57\n",
            "epoch 17, 700/1018 batches, training loss 4.53, training perplexity 92.95\n",
            "epoch 17, 800/1018 batches, training loss 4.38, training perplexity 79.45\n",
            "epoch 17, 900/1018 batches, training loss 4.45, training perplexity 85.46\n",
            "epoch 17, 1000/1018 batches, training loss 4.48, training perplexity 87.94\n",
            "\n",
            "epoch 17, validation loss 5.05, validation perplexity 155.78\n",
            "\n",
            "epoch 18, 100/1018 batches, training loss 4.55, training perplexity 94.40\n",
            "epoch 18, 200/1018 batches, training loss 4.47, training perplexity 87.58\n",
            "epoch 18, 300/1018 batches, training loss 4.49, training perplexity 88.81\n",
            "epoch 18, 400/1018 batches, training loss 4.49, training perplexity 88.76\n",
            "epoch 18, 500/1018 batches, training loss 4.49, training perplexity 89.39\n",
            "epoch 18, 600/1018 batches, training loss 4.49, training perplexity 89.07\n",
            "epoch 18, 700/1018 batches, training loss 4.51, training perplexity 90.78\n",
            "epoch 18, 800/1018 batches, training loss 4.36, training perplexity 78.16\n",
            "epoch 18, 900/1018 batches, training loss 4.43, training perplexity 84.04\n",
            "epoch 18, 1000/1018 batches, training loss 4.46, training perplexity 86.42\n",
            "\n",
            "epoch 18, validation loss 5.04, validation perplexity 153.82\n",
            "\n",
            "epoch 19, 100/1018 batches, training loss 4.53, training perplexity 93.05\n",
            "epoch 19, 200/1018 batches, training loss 4.46, training perplexity 86.12\n",
            "epoch 19, 300/1018 batches, training loss 4.47, training perplexity 87.36\n",
            "epoch 19, 400/1018 batches, training loss 4.47, training perplexity 87.33\n",
            "epoch 19, 500/1018 batches, training loss 4.47, training perplexity 87.74\n",
            "epoch 19, 600/1018 batches, training loss 4.47, training perplexity 87.60\n",
            "epoch 19, 700/1018 batches, training loss 4.49, training perplexity 89.29\n",
            "epoch 19, 800/1018 batches, training loss 4.34, training perplexity 76.86\n",
            "epoch 19, 900/1018 batches, training loss 4.41, training perplexity 82.43\n",
            "epoch 19, 1000/1018 batches, training loss 4.44, training perplexity 84.80\n",
            "\n",
            "epoch 19, validation loss 5.03, validation perplexity 153.23\n",
            "\n",
            "epoch 20, 100/1018 batches, training loss 4.51, training perplexity 91.12\n",
            "epoch 20, 200/1018 batches, training loss 4.44, training perplexity 85.09\n",
            "epoch 20, 300/1018 batches, training loss 4.45, training perplexity 85.95\n",
            "epoch 20, 400/1018 batches, training loss 4.45, training perplexity 85.78\n",
            "epoch 20, 500/1018 batches, training loss 4.46, training perplexity 86.36\n",
            "epoch 20, 600/1018 batches, training loss 4.46, training perplexity 86.63\n",
            "epoch 20, 700/1018 batches, training loss 4.48, training perplexity 88.07\n",
            "epoch 20, 800/1018 batches, training loss 4.33, training perplexity 75.68\n",
            "epoch 20, 900/1018 batches, training loss 4.40, training perplexity 81.36\n",
            "epoch 20, 1000/1018 batches, training loss 4.42, training perplexity 83.45\n",
            "\n",
            "epoch 20, validation loss 5.03, validation perplexity 152.78\n",
            "\n",
            "epoch 21, 100/1018 batches, training loss 4.50, training perplexity 89.96\n",
            "epoch 21, 200/1018 batches, training loss 4.43, training perplexity 83.80\n",
            "epoch 21, 300/1018 batches, training loss 4.45, training perplexity 85.20\n",
            "epoch 21, 400/1018 batches, training loss 4.44, training perplexity 85.08\n",
            "epoch 21, 500/1018 batches, training loss 4.45, training perplexity 85.51\n",
            "epoch 21, 600/1018 batches, training loss 4.45, training perplexity 85.36\n",
            "epoch 21, 700/1018 batches, training loss 4.47, training perplexity 87.14\n",
            "epoch 21, 800/1018 batches, training loss 4.32, training perplexity 74.89\n",
            "epoch 21, 900/1018 batches, training loss 4.39, training perplexity 80.34\n",
            "epoch 21, 1000/1018 batches, training loss 4.41, training perplexity 82.50\n",
            "\n",
            "epoch 21, validation loss 5.02, validation perplexity 151.50\n",
            "\n",
            "epoch 22, 100/1018 batches, training loss 4.49, training perplexity 89.13\n",
            "epoch 22, 200/1018 batches, training loss 4.42, training perplexity 83.15\n",
            "epoch 22, 300/1018 batches, training loss 4.43, training perplexity 84.13\n",
            "epoch 22, 400/1018 batches, training loss 4.43, training perplexity 84.22\n",
            "epoch 22, 500/1018 batches, training loss 4.44, training perplexity 84.79\n",
            "epoch 22, 600/1018 batches, training loss 4.44, training perplexity 84.50\n",
            "epoch 22, 700/1018 batches, training loss 4.46, training perplexity 86.20\n",
            "epoch 22, 800/1018 batches, training loss 4.30, training perplexity 74.01\n",
            "epoch 22, 900/1018 batches, training loss 4.37, training perplexity 79.34\n",
            "epoch 22, 1000/1018 batches, training loss 4.40, training perplexity 81.43\n",
            "\n",
            "epoch 22, validation loss 5.02, validation perplexity 151.61\n",
            "\n",
            "epoch 23, 100/1018 batches, training loss 4.48, training perplexity 88.45\n",
            "epoch 23, 200/1018 batches, training loss 4.41, training perplexity 82.05\n",
            "epoch 23, 300/1018 batches, training loss 4.42, training perplexity 83.50\n",
            "epoch 23, 400/1018 batches, training loss 4.42, training perplexity 83.10\n",
            "epoch 23, 500/1018 batches, training loss 4.43, training perplexity 84.09\n",
            "epoch 23, 600/1018 batches, training loss 4.43, training perplexity 83.71\n",
            "epoch 23, 700/1018 batches, training loss 4.45, training perplexity 85.69\n",
            "epoch 23, 800/1018 batches, training loss 4.29, training perplexity 73.33\n",
            "epoch 23, 900/1018 batches, training loss 4.36, training perplexity 78.54\n",
            "epoch 23, 1000/1018 batches, training loss 4.39, training perplexity 80.63\n",
            "\n",
            "epoch 23, validation loss 5.03, validation perplexity 152.33\n",
            "\n",
            "epoch 24, 100/1018 batches, training loss 4.47, training perplexity 87.66\n",
            "epoch 24, 200/1018 batches, training loss 4.40, training perplexity 81.46\n",
            "epoch 24, 300/1018 batches, training loss 4.41, training perplexity 82.65\n",
            "epoch 24, 400/1018 batches, training loss 4.41, training perplexity 82.67\n",
            "epoch 24, 500/1018 batches, training loss 4.42, training perplexity 83.04\n",
            "epoch 24, 600/1018 batches, training loss 4.42, training perplexity 82.94\n",
            "epoch 24, 700/1018 batches, training loss 4.44, training perplexity 84.57\n",
            "epoch 24, 800/1018 batches, training loss 4.29, training perplexity 72.65\n",
            "epoch 24, 900/1018 batches, training loss 4.36, training perplexity 78.13\n",
            "epoch 24, 1000/1018 batches, training loss 4.39, training perplexity 80.33\n",
            "\n",
            "epoch 24, validation loss 5.02, validation perplexity 150.98\n",
            "\n",
            "epoch 25, 100/1018 batches, training loss 4.47, training perplexity 87.22\n",
            "epoch 25, 200/1018 batches, training loss 4.39, training perplexity 80.93\n",
            "epoch 25, 300/1018 batches, training loss 4.41, training perplexity 82.17\n",
            "epoch 25, 400/1018 batches, training loss 4.41, training perplexity 82.22\n",
            "epoch 25, 500/1018 batches, training loss 4.41, training perplexity 82.61\n",
            "epoch 25, 600/1018 batches, training loss 4.41, training perplexity 82.45\n",
            "epoch 25, 700/1018 batches, training loss 4.43, training perplexity 84.28\n",
            "epoch 25, 800/1018 batches, training loss 4.28, training perplexity 72.18\n",
            "epoch 25, 900/1018 batches, training loss 4.35, training perplexity 77.78\n",
            "epoch 25, 1000/1018 batches, training loss 4.38, training perplexity 79.90\n",
            "\n",
            "epoch 25, validation loss 5.01, validation perplexity 150.42\n",
            "\n",
            "epoch 26, 100/1018 batches, training loss 4.46, training perplexity 86.60\n",
            "epoch 26, 200/1018 batches, training loss 4.39, training perplexity 80.70\n",
            "epoch 26, 300/1018 batches, training loss 4.40, training perplexity 81.75\n",
            "epoch 26, 400/1018 batches, training loss 4.40, training perplexity 81.73\n",
            "epoch 26, 500/1018 batches, training loss 4.41, training perplexity 82.13\n",
            "epoch 26, 600/1018 batches, training loss 4.41, training perplexity 82.10\n",
            "epoch 26, 700/1018 batches, training loss 4.43, training perplexity 83.97\n",
            "epoch 26, 800/1018 batches, training loss 4.28, training perplexity 71.97\n",
            "epoch 26, 900/1018 batches, training loss 4.35, training perplexity 77.43\n",
            "epoch 26, 1000/1018 batches, training loss 4.37, training perplexity 79.07\n",
            "\n",
            "epoch 26, validation loss 5.01, validation perplexity 150.44\n",
            "\n",
            "epoch 27, 100/1018 batches, training loss 4.46, training perplexity 86.09\n",
            "epoch 27, 200/1018 batches, training loss 4.39, training perplexity 80.44\n",
            "epoch 27, 300/1018 batches, training loss 4.40, training perplexity 81.15\n",
            "epoch 27, 400/1018 batches, training loss 4.40, training perplexity 81.45\n",
            "epoch 27, 500/1018 batches, training loss 4.41, training perplexity 81.96\n",
            "epoch 27, 600/1018 batches, training loss 4.40, training perplexity 81.70\n",
            "epoch 27, 700/1018 batches, training loss 4.42, training perplexity 83.39\n",
            "epoch 27, 800/1018 batches, training loss 4.27, training perplexity 71.56\n",
            "epoch 27, 900/1018 batches, training loss 4.34, training perplexity 76.95\n",
            "epoch 27, 1000/1018 batches, training loss 4.37, training perplexity 78.97\n",
            "\n",
            "epoch 27, validation loss 5.02, validation perplexity 150.97\n",
            "\n",
            "epoch 28, 100/1018 batches, training loss 4.46, training perplexity 86.07\n",
            "epoch 28, 200/1018 batches, training loss 4.38, training perplexity 80.14\n",
            "epoch 28, 300/1018 batches, training loss 4.39, training perplexity 81.02\n",
            "epoch 28, 400/1018 batches, training loss 4.40, training perplexity 81.24\n",
            "epoch 28, 500/1018 batches, training loss 4.40, training perplexity 81.29\n",
            "epoch 28, 600/1018 batches, training loss 4.40, training perplexity 81.18\n",
            "epoch 28, 700/1018 batches, training loss 4.42, training perplexity 83.14\n",
            "epoch 28, 800/1018 batches, training loss 4.27, training perplexity 71.35\n",
            "epoch 28, 900/1018 batches, training loss 4.34, training perplexity 76.49\n",
            "epoch 28, 1000/1018 batches, training loss 4.37, training perplexity 78.75\n",
            "\n",
            "epoch 28, validation loss 5.01, validation perplexity 150.09\n",
            "\n",
            "epoch 29, 100/1018 batches, training loss 4.45, training perplexity 85.78\n",
            "epoch 29, 200/1018 batches, training loss 4.38, training perplexity 80.02\n",
            "epoch 29, 300/1018 batches, training loss 4.39, training perplexity 80.75\n",
            "epoch 29, 400/1018 batches, training loss 4.39, training perplexity 80.82\n",
            "epoch 29, 500/1018 batches, training loss 4.40, training perplexity 81.54\n",
            "epoch 29, 600/1018 batches, training loss 4.40, training perplexity 81.20\n",
            "epoch 29, 700/1018 batches, training loss 4.42, training perplexity 82.87\n",
            "epoch 29, 800/1018 batches, training loss 4.26, training perplexity 71.14\n",
            "epoch 29, 900/1018 batches, training loss 4.34, training perplexity 76.56\n",
            "epoch 29, 1000/1018 batches, training loss 4.36, training perplexity 78.29\n",
            "\n",
            "epoch 29, validation loss 5.01, validation perplexity 149.72\n",
            "\n",
            "epoch 30, 100/1018 batches, training loss 4.45, training perplexity 85.60\n",
            "epoch 30, 200/1018 batches, training loss 4.38, training perplexity 79.85\n",
            "epoch 30, 300/1018 batches, training loss 4.39, training perplexity 80.71\n",
            "epoch 30, 400/1018 batches, training loss 4.39, training perplexity 80.52\n",
            "epoch 30, 500/1018 batches, training loss 4.40, training perplexity 81.07\n",
            "epoch 30, 600/1018 batches, training loss 4.39, training perplexity 80.87\n",
            "epoch 30, 700/1018 batches, training loss 4.42, training perplexity 82.82\n",
            "epoch 30, 800/1018 batches, training loss 4.26, training perplexity 70.86\n",
            "epoch 30, 900/1018 batches, training loss 4.33, training perplexity 76.30\n",
            "epoch 30, 1000/1018 batches, training loss 4.36, training perplexity 78.30\n",
            "\n",
            "epoch 30, validation loss 5.01, validation perplexity 149.22\n",
            "\n",
            "epoch 31, 100/1018 batches, training loss 4.45, training perplexity 85.69\n",
            "epoch 31, 200/1018 batches, training loss 4.38, training perplexity 79.60\n",
            "epoch 31, 300/1018 batches, training loss 4.39, training perplexity 80.63\n",
            "epoch 31, 400/1018 batches, training loss 4.39, training perplexity 80.36\n",
            "epoch 31, 500/1018 batches, training loss 4.39, training perplexity 81.01\n",
            "epoch 31, 600/1018 batches, training loss 4.39, training perplexity 80.75\n",
            "epoch 31, 700/1018 batches, training loss 4.41, training perplexity 82.49\n",
            "epoch 31, 800/1018 batches, training loss 4.26, training perplexity 70.79\n",
            "epoch 31, 900/1018 batches, training loss 4.33, training perplexity 76.29\n",
            "epoch 31, 1000/1018 batches, training loss 4.36, training perplexity 78.22\n",
            "\n",
            "epoch 31, validation loss 5.00, validation perplexity 148.87\n",
            "\n",
            "epoch 32, 100/1018 batches, training loss 4.45, training perplexity 85.48\n",
            "epoch 32, 200/1018 batches, training loss 4.38, training perplexity 79.49\n",
            "epoch 32, 300/1018 batches, training loss 4.39, training perplexity 80.56\n",
            "epoch 32, 400/1018 batches, training loss 4.38, training perplexity 80.20\n",
            "epoch 32, 500/1018 batches, training loss 4.39, training perplexity 80.79\n",
            "epoch 32, 600/1018 batches, training loss 4.39, training perplexity 80.76\n",
            "epoch 32, 700/1018 batches, training loss 4.41, training perplexity 82.21\n",
            "epoch 32, 800/1018 batches, training loss 4.26, training perplexity 70.92\n",
            "epoch 32, 900/1018 batches, training loss 4.33, training perplexity 76.00\n",
            "epoch 32, 1000/1018 batches, training loss 4.36, training perplexity 78.07\n",
            "\n",
            "epoch 32, validation loss 5.00, validation perplexity 148.42\n",
            "\n",
            "epoch 33, 100/1018 batches, training loss 4.45, training perplexity 85.31\n",
            "epoch 33, 200/1018 batches, training loss 4.38, training perplexity 79.48\n",
            "epoch 33, 300/1018 batches, training loss 4.39, training perplexity 80.65\n",
            "epoch 33, 400/1018 batches, training loss 4.39, training perplexity 80.46\n",
            "epoch 33, 500/1018 batches, training loss 4.39, training perplexity 80.25\n",
            "epoch 33, 600/1018 batches, training loss 4.39, training perplexity 80.81\n",
            "epoch 33, 700/1018 batches, training loss 4.41, training perplexity 82.13\n",
            "epoch 33, 800/1018 batches, training loss 4.26, training perplexity 70.60\n",
            "epoch 33, 900/1018 batches, training loss 4.33, training perplexity 76.16\n",
            "epoch 33, 1000/1018 batches, training loss 4.36, training perplexity 78.25\n",
            "\n",
            "epoch 33, validation loss 5.00, validation perplexity 148.55\n",
            "\n",
            "epoch 34, 100/1018 batches, training loss 4.45, training perplexity 85.25\n",
            "epoch 34, 200/1018 batches, training loss 4.37, training perplexity 79.30\n",
            "epoch 34, 300/1018 batches, training loss 4.39, training perplexity 80.45\n",
            "epoch 34, 400/1018 batches, training loss 4.38, training perplexity 80.08\n",
            "epoch 34, 500/1018 batches, training loss 4.39, training perplexity 80.65\n",
            "epoch 34, 600/1018 batches, training loss 4.39, training perplexity 80.54\n",
            "epoch 34, 700/1018 batches, training loss 4.41, training perplexity 82.14\n",
            "epoch 34, 800/1018 batches, training loss 4.26, training perplexity 70.66\n",
            "epoch 34, 900/1018 batches, training loss 4.33, training perplexity 76.00\n",
            "epoch 34, 1000/1018 batches, training loss 4.36, training perplexity 77.95\n",
            "\n",
            "epoch 34, validation loss 5.00, validation perplexity 148.42\n",
            "\n",
            "epoch 35, 100/1018 batches, training loss 4.45, training perplexity 85.45\n",
            "epoch 35, 200/1018 batches, training loss 4.37, training perplexity 79.30\n",
            "epoch 35, 300/1018 batches, training loss 4.39, training perplexity 80.38\n",
            "epoch 35, 400/1018 batches, training loss 4.38, training perplexity 79.99\n",
            "epoch 35, 500/1018 batches, training loss 4.39, training perplexity 80.57\n",
            "epoch 35, 600/1018 batches, training loss 4.39, training perplexity 80.25\n",
            "epoch 35, 700/1018 batches, training loss 4.41, training perplexity 82.32\n",
            "epoch 35, 800/1018 batches, training loss 4.26, training perplexity 70.62\n",
            "epoch 35, 900/1018 batches, training loss 4.33, training perplexity 75.62\n",
            "epoch 35, 1000/1018 batches, training loss 4.36, training perplexity 77.96\n",
            "\n",
            "epoch 35, validation loss 5.00, validation perplexity 147.77\n",
            "\n",
            "epoch 36, 100/1018 batches, training loss 4.45, training perplexity 85.41\n",
            "epoch 36, 200/1018 batches, training loss 4.37, training perplexity 79.35\n",
            "epoch 36, 300/1018 batches, training loss 4.39, training perplexity 80.51\n",
            "epoch 36, 400/1018 batches, training loss 4.39, training perplexity 80.24\n",
            "epoch 36, 500/1018 batches, training loss 4.39, training perplexity 80.56\n",
            "epoch 36, 600/1018 batches, training loss 4.39, training perplexity 80.57\n",
            "epoch 36, 700/1018 batches, training loss 4.41, training perplexity 82.18\n",
            "epoch 36, 800/1018 batches, training loss 4.26, training perplexity 70.71\n",
            "epoch 36, 900/1018 batches, training loss 4.33, training perplexity 75.74\n",
            "epoch 36, 1000/1018 batches, training loss 4.36, training perplexity 78.00\n",
            "\n",
            "epoch 36, validation loss 4.99, validation perplexity 147.60\n",
            "\n",
            "epoch 37, 100/1018 batches, training loss 4.44, training perplexity 85.18\n",
            "epoch 37, 200/1018 batches, training loss 4.38, training perplexity 79.64\n",
            "epoch 37, 300/1018 batches, training loss 4.39, training perplexity 80.70\n",
            "epoch 37, 400/1018 batches, training loss 4.38, training perplexity 80.22\n",
            "epoch 37, 500/1018 batches, training loss 4.39, training perplexity 80.81\n",
            "epoch 37, 600/1018 batches, training loss 4.39, training perplexity 80.70\n",
            "epoch 37, 700/1018 batches, training loss 4.41, training perplexity 82.26\n",
            "epoch 37, 800/1018 batches, training loss 4.26, training perplexity 70.63\n",
            "epoch 37, 900/1018 batches, training loss 4.33, training perplexity 75.85\n",
            "epoch 37, 1000/1018 batches, training loss 4.36, training perplexity 78.04\n",
            "\n",
            "epoch 37, validation loss 4.99, validation perplexity 147.27\n",
            "\n",
            "epoch 38, 100/1018 batches, training loss 4.45, training perplexity 85.42\n",
            "epoch 38, 200/1018 batches, training loss 4.38, training perplexity 79.69\n",
            "epoch 38, 300/1018 batches, training loss 4.39, training perplexity 80.77\n",
            "epoch 38, 400/1018 batches, training loss 4.38, training perplexity 80.09\n",
            "epoch 38, 500/1018 batches, training loss 4.39, training perplexity 80.76\n",
            "epoch 38, 600/1018 batches, training loss 4.39, training perplexity 80.52\n",
            "epoch 38, 700/1018 batches, training loss 4.41, training perplexity 82.31\n",
            "epoch 38, 800/1018 batches, training loss 4.26, training perplexity 70.60\n",
            "epoch 38, 900/1018 batches, training loss 4.33, training perplexity 75.85\n",
            "epoch 38, 1000/1018 batches, training loss 4.36, training perplexity 78.02\n",
            "\n",
            "epoch 38, validation loss 4.99, validation perplexity 146.94\n",
            "\n",
            "epoch 39, 100/1018 batches, training loss 4.45, training perplexity 85.49\n",
            "epoch 39, 200/1018 batches, training loss 4.38, training perplexity 79.64\n",
            "epoch 39, 300/1018 batches, training loss 4.39, training perplexity 80.55\n",
            "epoch 39, 400/1018 batches, training loss 4.39, training perplexity 80.74\n",
            "epoch 39, 500/1018 batches, training loss 4.39, training perplexity 81.02\n",
            "epoch 39, 600/1018 batches, training loss 4.39, training perplexity 80.57\n",
            "epoch 39, 700/1018 batches, training loss 4.41, training perplexity 82.29\n",
            "epoch 39, 800/1018 batches, training loss 4.26, training perplexity 70.56\n",
            "epoch 39, 900/1018 batches, training loss 4.33, training perplexity 75.80\n",
            "epoch 39, 1000/1018 batches, training loss 4.36, training perplexity 78.22\n",
            "\n",
            "epoch 39, validation loss 4.99, validation perplexity 146.56\n",
            "\n",
            "epoch 40, 100/1018 batches, training loss 4.45, training perplexity 85.51\n",
            "epoch 40, 200/1018 batches, training loss 4.38, training perplexity 79.71\n",
            "epoch 40, 300/1018 batches, training loss 4.39, training perplexity 80.85\n",
            "epoch 40, 400/1018 batches, training loss 4.39, training perplexity 80.33\n",
            "epoch 40, 500/1018 batches, training loss 4.40, training perplexity 81.06\n",
            "epoch 40, 600/1018 batches, training loss 4.39, training perplexity 80.72\n",
            "epoch 40, 700/1018 batches, training loss 4.41, training perplexity 82.44\n",
            "epoch 40, 800/1018 batches, training loss 4.26, training perplexity 70.71\n",
            "epoch 40, 900/1018 batches, training loss 4.33, training perplexity 75.62\n",
            "epoch 40, 1000/1018 batches, training loss 4.36, training perplexity 78.31\n",
            "\n",
            "epoch 40, validation loss 4.99, validation perplexity 146.48\n",
            "\n",
            "epoch 41, 100/1018 batches, training loss 4.45, training perplexity 85.46\n",
            "epoch 41, 200/1018 batches, training loss 4.38, training perplexity 79.80\n",
            "epoch 41, 300/1018 batches, training loss 4.39, training perplexity 80.87\n",
            "epoch 41, 400/1018 batches, training loss 4.39, training perplexity 80.55\n",
            "epoch 41, 500/1018 batches, training loss 4.40, training perplexity 81.24\n",
            "epoch 41, 600/1018 batches, training loss 4.39, training perplexity 80.77\n",
            "epoch 41, 700/1018 batches, training loss 4.41, training perplexity 82.39\n",
            "epoch 41, 800/1018 batches, training loss 4.26, training perplexity 70.80\n",
            "epoch 41, 900/1018 batches, training loss 4.33, training perplexity 75.91\n",
            "epoch 41, 1000/1018 batches, training loss 4.36, training perplexity 78.12\n",
            "\n",
            "epoch 41, validation loss 4.99, validation perplexity 146.22\n",
            "\n",
            "epoch 42, 100/1018 batches, training loss 4.45, training perplexity 85.54\n",
            "epoch 42, 200/1018 batches, training loss 4.38, training perplexity 80.06\n",
            "epoch 42, 300/1018 batches, training loss 4.39, training perplexity 80.92\n",
            "epoch 42, 400/1018 batches, training loss 4.39, training perplexity 80.79\n",
            "epoch 42, 500/1018 batches, training loss 4.40, training perplexity 81.53\n",
            "epoch 42, 600/1018 batches, training loss 4.39, training perplexity 80.73\n",
            "epoch 42, 700/1018 batches, training loss 4.41, training perplexity 82.44\n",
            "epoch 42, 800/1018 batches, training loss 4.26, training perplexity 70.78\n",
            "epoch 42, 900/1018 batches, training loss 4.33, training perplexity 75.99\n",
            "epoch 42, 1000/1018 batches, training loss 4.36, training perplexity 78.34\n",
            "\n",
            "epoch 42, validation loss 4.98, validation perplexity 146.10\n",
            "\n",
            "epoch 43, 100/1018 batches, training loss 4.45, training perplexity 85.71\n",
            "epoch 43, 200/1018 batches, training loss 4.38, training perplexity 79.99\n",
            "epoch 43, 300/1018 batches, training loss 4.39, training perplexity 81.01\n",
            "epoch 43, 400/1018 batches, training loss 4.39, training perplexity 80.35\n",
            "epoch 43, 500/1018 batches, training loss 4.40, training perplexity 81.45\n",
            "epoch 43, 600/1018 batches, training loss 4.40, training perplexity 81.11\n",
            "epoch 43, 700/1018 batches, training loss 4.42, training perplexity 82.76\n",
            "epoch 43, 800/1018 batches, training loss 4.26, training perplexity 70.86\n",
            "epoch 43, 900/1018 batches, training loss 4.33, training perplexity 75.91\n",
            "epoch 43, 1000/1018 batches, training loss 4.36, training perplexity 78.44\n",
            "\n",
            "epoch 43, validation loss 4.98, validation perplexity 145.84\n",
            "\n",
            "epoch 44, 100/1018 batches, training loss 4.45, training perplexity 85.86\n",
            "epoch 44, 200/1018 batches, training loss 4.38, training perplexity 79.97\n",
            "epoch 44, 300/1018 batches, training loss 4.39, training perplexity 80.96\n",
            "epoch 44, 400/1018 batches, training loss 4.39, training perplexity 80.58\n",
            "epoch 44, 500/1018 batches, training loss 4.40, training perplexity 81.72\n",
            "epoch 44, 600/1018 batches, training loss 4.39, training perplexity 81.04\n",
            "epoch 44, 700/1018 batches, training loss 4.42, training perplexity 82.73\n",
            "epoch 44, 800/1018 batches, training loss 4.26, training perplexity 70.93\n",
            "epoch 44, 900/1018 batches, training loss 4.33, training perplexity 76.32\n",
            "epoch 44, 1000/1018 batches, training loss 4.36, training perplexity 78.40\n",
            "\n",
            "epoch 44, validation loss 4.98, validation perplexity 145.44\n",
            "\n",
            "epoch 45, 100/1018 batches, training loss 4.46, training perplexity 86.12\n",
            "epoch 45, 200/1018 batches, training loss 4.39, training perplexity 80.32\n",
            "epoch 45, 300/1018 batches, training loss 4.40, training perplexity 81.21\n",
            "epoch 45, 400/1018 batches, training loss 4.39, training perplexity 80.74\n",
            "epoch 45, 500/1018 batches, training loss 4.40, training perplexity 81.63\n",
            "epoch 45, 600/1018 batches, training loss 4.39, training perplexity 81.02\n",
            "epoch 45, 700/1018 batches, training loss 4.42, training perplexity 82.75\n",
            "epoch 45, 800/1018 batches, training loss 4.26, training perplexity 70.95\n",
            "epoch 45, 900/1018 batches, training loss 4.33, training perplexity 76.15\n",
            "epoch 45, 1000/1018 batches, training loss 4.36, training perplexity 78.48\n",
            "\n",
            "epoch 45, validation loss 4.98, validation perplexity 145.34\n",
            "\n",
            "epoch 46, 100/1018 batches, training loss 4.45, training perplexity 86.05\n",
            "epoch 46, 200/1018 batches, training loss 4.38, training perplexity 80.22\n",
            "epoch 46, 300/1018 batches, training loss 4.40, training perplexity 81.38\n",
            "epoch 46, 400/1018 batches, training loss 4.39, training perplexity 80.84\n",
            "epoch 46, 500/1018 batches, training loss 4.40, training perplexity 81.78\n",
            "epoch 46, 600/1018 batches, training loss 4.40, training perplexity 81.41\n",
            "epoch 46, 700/1018 batches, training loss 4.42, training perplexity 83.20\n",
            "epoch 46, 800/1018 batches, training loss 4.26, training perplexity 71.06\n",
            "epoch 46, 900/1018 batches, training loss 4.34, training perplexity 76.54\n",
            "epoch 46, 1000/1018 batches, training loss 4.36, training perplexity 78.55\n",
            "\n",
            "epoch 46, validation loss 4.98, validation perplexity 145.04\n",
            "\n",
            "epoch 47, 100/1018 batches, training loss 4.45, training perplexity 85.98\n",
            "epoch 47, 200/1018 batches, training loss 4.39, training perplexity 80.34\n",
            "epoch 47, 300/1018 batches, training loss 4.40, training perplexity 81.53\n",
            "epoch 47, 400/1018 batches, training loss 4.39, training perplexity 80.71\n",
            "epoch 47, 500/1018 batches, training loss 4.40, training perplexity 81.70\n",
            "epoch 47, 600/1018 batches, training loss 4.40, training perplexity 81.52\n",
            "epoch 47, 700/1018 batches, training loss 4.42, training perplexity 83.12\n",
            "epoch 47, 800/1018 batches, training loss 4.26, training perplexity 71.02\n",
            "epoch 47, 900/1018 batches, training loss 4.34, training perplexity 76.54\n",
            "epoch 47, 1000/1018 batches, training loss 4.37, training perplexity 78.69\n",
            "\n",
            "epoch 47, validation loss 4.98, validation perplexity 144.87\n",
            "\n",
            "epoch 48, 100/1018 batches, training loss 4.46, training perplexity 86.44\n",
            "epoch 48, 200/1018 batches, training loss 4.39, training perplexity 80.77\n",
            "epoch 48, 300/1018 batches, training loss 4.40, training perplexity 81.70\n",
            "epoch 48, 400/1018 batches, training loss 4.39, training perplexity 80.83\n",
            "epoch 48, 500/1018 batches, training loss 4.40, training perplexity 81.67\n",
            "epoch 48, 600/1018 batches, training loss 4.40, training perplexity 81.39\n",
            "epoch 48, 700/1018 batches, training loss 4.42, training perplexity 83.23\n",
            "epoch 48, 800/1018 batches, training loss 4.27, training perplexity 71.32\n",
            "epoch 48, 900/1018 batches, training loss 4.34, training perplexity 76.80\n",
            "epoch 48, 1000/1018 batches, training loss 4.36, training perplexity 78.62\n",
            "\n",
            "epoch 48, validation loss 4.97, validation perplexity 144.70\n",
            "\n",
            "epoch 49, 100/1018 batches, training loss 4.45, training perplexity 85.86\n",
            "epoch 49, 200/1018 batches, training loss 4.39, training perplexity 80.67\n",
            "epoch 49, 300/1018 batches, training loss 4.41, training perplexity 82.05\n",
            "epoch 49, 400/1018 batches, training loss 4.40, training perplexity 81.16\n",
            "epoch 49, 500/1018 batches, training loss 4.40, training perplexity 81.85\n",
            "epoch 49, 600/1018 batches, training loss 4.40, training perplexity 81.54\n",
            "epoch 49, 700/1018 batches, training loss 4.42, training perplexity 83.22\n",
            "epoch 49, 800/1018 batches, training loss 4.27, training perplexity 71.40\n",
            "epoch 49, 900/1018 batches, training loss 4.34, training perplexity 76.69\n",
            "epoch 49, 1000/1018 batches, training loss 4.37, training perplexity 78.73\n",
            "\n",
            "epoch 49, validation loss 4.97, validation perplexity 144.44\n",
            "\n",
            "epoch 50, 100/1018 batches, training loss 4.46, training perplexity 86.10\n",
            "epoch 50, 200/1018 batches, training loss 4.39, training perplexity 80.52\n",
            "epoch 50, 300/1018 batches, training loss 4.40, training perplexity 81.84\n",
            "epoch 50, 400/1018 batches, training loss 4.39, training perplexity 81.01\n",
            "epoch 50, 500/1018 batches, training loss 4.41, training perplexity 81.96\n",
            "epoch 50, 600/1018 batches, training loss 4.40, training perplexity 81.70\n",
            "epoch 50, 700/1018 batches, training loss 4.42, training perplexity 83.11\n",
            "epoch 50, 800/1018 batches, training loss 4.27, training perplexity 71.63\n",
            "epoch 50, 900/1018 batches, training loss 4.34, training perplexity 76.52\n",
            "epoch 50, 1000/1018 batches, training loss 4.36, training perplexity 78.64\n",
            "\n",
            "epoch 50, validation loss 4.97, validation perplexity 144.30\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "testing_loss = eval_model(best_model_so_far, testing_data)\n",
        "print(f\"testing loss {testing_loss:.2f}, testing perplexity {math.exp(testing_loss):.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uiqfWcjr18kn",
        "outputId": "8f7f7d8b-a127-4e96-f7ca-945f86949f59"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "testing loss 4.91, testing perplexity 135.82\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mdl_pth = './transformer.pth'\n",
        "torch.save(best_model_so_far.state_dict(), mdl_pth)"
      ],
      "metadata": {
        "id": "PkQGQI1Z1_xM"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load the best trained model\n",
        "transformer_cached = Transformer(num_tokens, embedding_size, num_heads, num_hidden_params, num_layers,\n",
        "                                     dropout).to(device)\n",
        "transformer_cached.load_state_dict(torch.load(mdl_pth))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sAFLAEzQ2A4U",
        "outputId": "37c9a345-0325-4a63-89fe-3e55afe3f801"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/transformer.py:282: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
            "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ln = 10\n",
        "sntc = 'It will _'\n",
        "sntc_split = sntc.split()\n",
        "with torch.no_grad():\n",
        "    for i in range(ln):\n",
        "        sntc = ' '.join(sntc_split)\n",
        "        txt_ds = TEXT.numericalize([sntc_split])\n",
        "        num_b = txt_ds.size(0)\n",
        "        txt_ds = txt_ds.narrow(0, 0, num_b)\n",
        "        txt_ds = txt_ds.view(1, -1).t().contiguous().to(device)\n",
        "        ev_X, _ = return_batch(txt_ds, i+1)\n",
        "        op = transformer_cached(ev_X)\n",
        "        op_flat = op.view(-1, num_tokens)\n",
        "        res = TEXT.vocab.itos[op_flat.argmax(1)[0]]\n",
        "        sntc_split.insert(-1, res)\n",
        "print(sntc[:-2])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fykx4v922CWd",
        "outputId": "81e0849a-7897-4abc-81b1-a6d198a33785"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It will be used to the song song , and the\n"
          ]
        }
      ]
    }
  ]
}